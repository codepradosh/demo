#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Push categorized incident CSV file to PostgreSQL table Incident_Data
Usage: python push_incidents_to_postgres.py <csv_file_path> [--truncate]
"""

import os
import sys
import pandas as pd
import traceback
import logging
from datetime import datetime
import argparse
from urllib.parse import quote_plus

# PostgreSQL client
try:
    import psycopg2
    from sqlalchemy import create_engine, text
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("ERROR: PostgreSQL libraries not available.")
    print("Please install: pip install psycopg2-binary sqlalchemy pandas")
    sys.exit(1)

# ------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------
def setup_logging():
    """Set up logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

# ------------------------------------------------------------------------
# PostgreSQL Configuration
# ------------------------------------------------------------------------
POSTGRES_CONFIG = {
    'host': os.getenv('PGHOST', 'xd1e1159676dev.ubscloud-prod.msad.ubs.net'),
    'port': os.getenv('PGPORT', '5432'),
    'database': os.getenv('PGDATABASE', 'service_automation_db'),
    'user': os.getenv('PGUSER', 'powerbi_user'),
    'password': os.getenv('PGPASSWORD', 'Report@123'),
    'table_name': 'Incident_Data'
}

# ------------------------------------------------------------------------
# Main Function
# ------------------------------------------------------------------------
def push_incidents_to_postgresql(csv_file_path, table_name=None, database=None, 
                                 user=None, host=None, port=None, password=None,
                                 truncate=False):
    """
    Push categorized incident CSV file to PostgreSQL table.
    
    Parameters
    ----------
    csv_file_path : str
        Path to the categorized CSV file to import
    table_name : str, optional
        Target table name (default: 'Incident_Data')
    database : str, optional
        Database name (default: from config or 'service_automation_db')
    user : str, optional
        PostgreSQL username (default: from config)
    host : str, optional
        PostgreSQL host (default: from config)
    port : str, optional
        PostgreSQL port (default: from config or '5432')
    password : str, optional
        PostgreSQL password (default: from environment variable PGPASSWORD)
    truncate : bool, optional
        If True, truncate table before inserting (default: False)
        
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    if not POSTGRES_AVAILABLE:
        logging.error("‚úó PostgreSQL libraries not available. Cannot export to PostgreSQL.")
        return False
    
    if not os.path.exists(csv_file_path):
        logging.error(f"‚úó CSV file not found: {csv_file_path}")
        return False
    
    # Use provided values or fall back to config/environment
    table_name = table_name or POSTGRES_CONFIG['table_name']
    database = database or POSTGRES_CONFIG['database']
    user = user or POSTGRES_CONFIG['user']
    host = host or POSTGRES_CONFIG['host']
    port = port or POSTGRES_CONFIG['port']
    password = password or POSTGRES_CONFIG['password']
    
    try:
        logging.info("="*80)
        logging.info("üì§ Pushing Incident CSV to PostgreSQL")
        logging.info("="*80)
        logging.info(f"   CSV file: {csv_file_path}")
        logging.info(f"   Database: {database}")
        logging.info(f"   Table: {table_name}")
        logging.info(f"   Host: {host}:{port}")
        logging.info(f"   User: {user}")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info("="*80)
        
        # Read CSV file
        logging.info(f"\n‚è≥ Reading CSV file...")
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_file_path, encoding='latin-1')
            except:
                df = pd.read_csv(csv_file_path, encoding='cp1252')
        
        logging.info(f"‚úì Read {len(df):,} rows from CSV")
        logging.info(f"‚úì Columns: {len(df.columns)} ({', '.join(df.columns[:5])}...)")
        
        # Convert date columns to datetime if they're strings
        date_columns = ['opened_at', 'closed_at', 'resolved_at', 'sys_created_on', 'sys_updated_on']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                logging.info(f"‚úì Converted {col} to datetime")
        
        # Convert boolean columns
        bool_columns = ['active', 'made_sla']
        for col in bool_columns:
            if col in df.columns:
                df[col] = df[col].astype(bool)
        
        # Convert integer columns
        int_columns = ['reassignment_count', 'reopen_count']
        for col in int_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
        
        # Create connection string with URL-encoded password
        # Fix: Use quote_plus to properly encode password (handles @, #, etc.)
        if password:
            # URL-encode the password to handle special characters like @
            encoded_password = quote_plus(password)
            connection_string = f"postgresql://{user}:{encoded_password}@{host}:{port}/{database}"
        else:
            # Try without password (trust authentication or .pgpass)
            connection_string = f"postgresql://{user}@{host}:{port}/{database}"
        
        # Create SQLAlchemy engine
        logging.info(f"\n‚è≥ Connecting to PostgreSQL...")
        engine = create_engine(connection_string, pool_pre_ping=True)
        
        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logging.info(f"‚úì Connected to PostgreSQL successfully")
        
        # Check if table exists
        logging.info(f"\n‚è≥ Checking if table '{table_name}' exists...")
        with engine.connect() as conn:
            result = conn.execute(text(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """))
            table_exists = result.fetchone()[0]
        
        if not table_exists:
            logging.error(f"‚úó Table '{table_name}' does not exist in database '{database}'")
            logging.error(f"  Please create the table first.")
            engine.dispose()
            return False
        
        logging.info(f"‚úì Table '{table_name}' exists")
        
        # Get table column names
        logging.info(f"\n‚è≥ Fetching table column names...")
        with engine.connect() as conn:
            result = conn.execute(text(f"""
                SELECT column_name 
                FROM information_schema.columns
                WHERE table_name = '{table_name}'
                AND table_schema = 'public'
                ORDER BY ordinal_position;
            """))
            table_columns = [row[0] for row in result.fetchall()]
        
        col_count = len(table_columns)
        logging.info(f"‚úì Table has {col_count} columns")
        
        # Validate CSV columns match table columns
        logging.info(f"\n‚è≥ Validating column names...")
        
        csv_columns = [col for col in df.columns]
        table_columns_lower = {col.lower(): col for col in table_columns}
        
        # Create column mapping (case-insensitive match)
        column_mapping = {}
        for csv_col in csv_columns:
            csv_col_lower = csv_col.lower()
            if csv_col_lower in table_columns_lower:
                table_col = table_columns_lower[csv_col_lower]
                if csv_col != table_col:
                    column_mapping[csv_col] = table_col
        
        if column_mapping:
            df.rename(columns=column_mapping, inplace=True)
            logging.info(f"‚úì Renamed {len(column_mapping)} columns to match table case")
        
        # Find missing columns in CSV
        metadata_cols = {'created_at', 'updated_at'}
        missing_in_csv = []
        for table_col in table_columns:
            if table_col.lower() not in [c.lower() for c in df.columns] and table_col not in metadata_cols:
                missing_in_csv.append(table_col)
                df[table_col] = None
        
        if missing_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV is missing {len(missing_in_csv)} table columns (set to NULL):")
            for col in missing_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(missing_in_csv) > 10:
                logging.warning(f"   ... and {len(missing_in_csv) - 10} more")
        
        # Find extra columns in CSV (not in table)
        extra_in_csv = []
        for csv_col in df.columns:
            if csv_col not in table_columns and csv_col not in metadata_cols:
                extra_in_csv.append(csv_col)
        
        if extra_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV has {len(extra_in_csv)} extra columns (will be ignored):")
            for col in extra_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(extra_in_csv) > 10:
                logging.warning(f"   ... and {len(extra_in_csv) - 10} more")
        
        # Filter DataFrame to only include columns that exist in table
        columns_to_insert = [col for col in table_columns if col not in metadata_cols]
        df_final = df[[col for col in columns_to_insert if col in df.columns]].copy()
        
        # Check existing row count
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
            existing_rows = result.fetchone()[0]
        logging.info(f"‚úì Table currently has {existing_rows:,} rows")
        
        # Truncate table if requested
        if truncate:
            logging.info(f"\n‚è≥ Truncating table '{table_name}'...")
            with engine.connect() as conn:
                conn.execute(text(f'TRUNCATE TABLE "{table_name}";'))
                conn.commit()
            logging.info(f"‚úì Table truncated")
        
        # Push to database
        logging.info(f"\n‚è≥ Writing {len(df_final):,} rows to PostgreSQL table '{table_name}'...")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info(f"   Columns: {len(df_final.columns)} columns matching table structure")
        logging.info(f"   Chunk size: 1000 rows per batch")
        
        df_final.to_sql(
            name=table_name,
            con=engine,
            schema='public',
            if_exists='append',  # Append since we truncated if needed
            index=False,
            method='multi',  # Use multi-row INSERT for better performance
            chunksize=1000
        )
        
        engine.dispose()
        
        # Verify final row count
        engine = create_engine(connection_string, pool_pre_ping=True)
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
            final_rows = result.fetchone()[0]
        engine.dispose()
        
        logging.info(f"\n{'='*80}")
        logging.info("‚úÖ SUCCESS!")
        logging.info(f"{'='*80}")
        logging.info(f"‚úì Wrote {len(df_final):,} rows to PostgreSQL")
        logging.info(f"‚úì Database: {database}")
        logging.info(f"‚úì Table: {table_name}")
        logging.info(f"‚úì Total rows in table now: {final_rows:,}")
        if not truncate:
            logging.info(f"   (Previous: {existing_rows:,}, Added: {len(df_final):,})")
        logging.info(f"{'='*80}\n")
        
        return True
        
    except Exception as e:
        logging.error(f"\n‚úó Failed to export to PostgreSQL: {e}")
        logging.error(traceback.format_exc())
        return False

def main():
    """Main execution function."""
    setup_logging()
    
    parser = argparse.ArgumentParser(
        description='Push categorized incident CSV to PostgreSQL',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv --truncate

Environment variables (optional):
  PGHOST=localhost
  PGPORT=5432
  PGDATABASE=service_automation_db
  PGUSER=postgres
  PGPASSWORD=your_password

Or use .pgpass file for password-less authentication
        """
    )
    
    parser.add_argument('csv_file', help='Path to categorized incident CSV file')
    parser.add_argument('--truncate', action='store_true', 
                       help='Truncate table before inserting (overwrite mode)')
    parser.add_argument('--table', type=str, default=None,
                       help='Table name (default: Incident_Data)')
    parser.add_argument('--database', type=str, default=None,
                       help='Database name (default: from config)')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.csv_file):
        logging.error(f"‚úó CSV file not found: {args.csv_file}")
        sys.exit(1)
    
    # Push to PostgreSQL
    success = push_incidents_to_postgresql(
        args.csv_file,
        table_name=args.table,
        database=args.database,
        truncate=args.truncate
    )
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()

