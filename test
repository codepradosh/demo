#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Complete Incident Data Pipeline
Flow:
1. Pull data from Atlas and gsnow (2024 to current)
2. Merge them properly (Atlas PRIMARY, gsnow supplementary)
3. Fix opened_at issue (use created_on as fallback for gsnow)
4. Export to CSV
5. Run categorization script (category_latest.py) with GPT clustering
6. Load categorized data to PostgreSQL
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import urllib3
import datetime
import traceback
import re
import subprocess
import tempfile
from pathlib import Path
from dotenv import load_dotenv
from pystarburst import Session
from trino.auth import OAuth2Authentication
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.types import Integer, BigInteger, Float, Boolean, DateTime, Text, String, Date
from urllib.parse import quote_plus
from tqdm import tqdm

# Disable SSL warnings
urllib3.disable_warnings()

# PostgreSQL imports
try:
    import psycopg2
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("Warning: PostgreSQL libraries not available. Install with: pip install psycopg2-binary sqlalchemy")

# ------------------------------------------------------------------------
# Configuration
# ------------------------------------------------------------------------
# Atlas connection details
ATLAS_CONNECTION_DETAILS = [
    {
        "connectionString": "Driver={Starburst ODBC Driver};Host=app-neu.starburst-prod.azpriv-cloud.ubs.net;Port=443;AuthenticationType={LDAP Authentication};SSL=1",
        "catalogName": "ipm_service_management",
        "schemaName": "ipm_service_management_schema",
        "tableName": "ipm_incident_flat",
        "dataproductId": "281244d9-2e0f-4c77-ba20-964da41911b1",
        "outputportId": "df9f6069-8ea1-4f0d-b01a-76e5ba693ca6"
    }
]

# gsnow connection details
GSNOW_CONNECTION_DETAILS = [
    {
        "connectionString": "Driver={Starburst ODBC Driver};Host=app-neu.starburst-prod.azpriv-cloud.ubs.net;Port=443;AuthenticationType=LDAP Authentication;SSL=1",
        "catalogName": "ts_dwh_gsnow",
        "schemaName": "ts_dwh_gsnow_schema",
        "tableName": "gsnow_inc_main_unstrreq_36mth",
        "dataproductId": "72567983-5ac4-405b-a6f8-6b9bd79244fe",
        "outputportId": "1709c349-7dae-492f-934e-2ce4bb5c5870"
    }
]

# PostgreSQL Configuration
POSTGRES_CONFIG = {
    'host': os.getenv('PGHOST', 'xd1e1159676dev.ubscloud-prod.msad.ubs.net'),
    'port': os.getenv('PGPORT', '5432'),
    'database': os.getenv('PGDATABASE', 'service_automation_db'),
    'user': os.getenv('PGUSER', 'powerbi_user'),
    'password': os.getenv('PGPASSWORD', 'Report@123'),
    'table_name': 'Incident_Data'  # Capitalized table name
}

# Azure OpenAI Configuration (for categorization)
AZURE_ENDPOINT = "https://dh-automation-1.openai.azure.com/openai/v1"
AZURE_API_VERSION = "2024-02-15-preview"
GPT_MODEL = "gpt-4o-mini"
AZURE_API_KEY = os.getenv('AZURE_OPENAI_API_KEY', '')

# ------------------------------------------------------------------------
# Database Connection Functions
# ------------------------------------------------------------------------
def create_atlas_session():
    """Create PyStarburst session for IPM Service Management catalog (Atlas)"""
    connection_details = ATLAS_CONNECTION_DETAILS
    
    session = None
    catalog_name = None
    
    for platform_connection_info in connection_details:
        if platform_connection_info.get("catalogName") == "ipm_service_management":
            connection_string_list = platform_connection_info["connectionString"].split(";")
            catalog_name = platform_connection_info["catalogName"]
            
            starburst_connection_details = {}
            for elem in connection_string_list:
                if "=" in elem:
                    key, value = elem.split("=", 1)
                    starburst_connection_details[key.strip()] = value.strip()
            
            host = starburst_connection_details.get("Host")
            port = starburst_connection_details.get("Port", "443")
            
            db_parameters = {
                "host": host,
                "port": port,
                "http_scheme": "https",
                "verify": False,
                "auth": OAuth2Authentication(),
                "catalog": catalog_name
            }
            session = Session.builder.configs(db_parameters).create()
            break
    
    return session, catalog_name

def create_gsnow_session():
    """Create PyStarburst session for ts_dwh_gsnow catalog"""
    connection_details = GSNOW_CONNECTION_DETAILS
    
    session = None
    catalog_name = None
    
    for platform_connection_info in connection_details:
        if platform_connection_info.get("catalogName") == "ts_dwh_gsnow":
            connection_string_list = platform_connection_info["connectionString"].split(";")
            catalog_name = platform_connection_info["catalogName"]
            
            starburst_connection_details = {}
            for elem in connection_string_list:
                if "=" in elem:
                    key, value = elem.split("=", 1)
                    starburst_connection_details[key.strip()] = value.strip()
            
            host = starburst_connection_details.get("Host")
            port = starburst_connection_details.get("Port", "443")
            
            db_parameters = {
                "host": host,
                "port": port,
                "http_scheme": "https",
                "verify": False,
                "auth": OAuth2Authentication(),
                "catalog": catalog_name
            }
            session = Session.builder.configs(db_parameters).create()
            break
    
    return session, catalog_name

# ------------------------------------------------------------------------
# Column Mapping
# ------------------------------------------------------------------------
def get_column_mapping():
    """Define column mapping between Atlas and gsnow sources"""
    mapping = {
        "inc_number": "incident_number",
        "assigned_grp": "assignment_group",
        "created_on": "sys_created_on",
        "opened_at": "opened_at",
        "resolved_at": "resolved_at",
        "closed_at": "closed_at",
        "updated_on": "sys_updated_on",
        "inc_priority": "priority",
        "severity": "severity",
        "urgency": "urgency",
        "business_impact": "business_impact",
        "operational_impact": "u_operational_impact",
        "category": "category",
        "sub-category": "subcategory",
        "type": "contact_type",
        "contact_type": "contact_type",
        "short_description": "short_description",
        "description": "description",
        "solution": "close_notes",
        "service_owner_grp": "business_service",
        "service_impacted": "cmdb_ci",
        "service_impacted_id": "cmdb_ci_sys_id",
        "creator_gpn": "opened_by_gpn",
        "creator_bus_name": "opened_by_user_name",
        "impacted_country": "u_impacted_country",
        "business_cluster": "u_business_cluster",
        "business_cluster_owner": "u_business_cluster_owner",
        "business_cluster_owner_gpn": "u_business_cluster_owner_gpn",
        "service_environment": "u_service_environment",
        "service_division": "service_division",
        "service_division_id": "service_division_id",
        "business_stream": "u_business_stream",
        "business_stream_prg_mgr": "u_business_stream_prg_mgr",
        "business_stream_prg_mgr_gpn": "u_business_stream_prg_mgr_gpn",
        "business_stream_id": "u_business_stream_id",
        "it_sector": "u_it_sector",
        "it_sector_cio": "u_it_sector_cio",
        "solution_domain": "u_solution_domain",
        "solution_domain_id": "u_solution_domain_id",
        "solution_domain_mgr": "u_solution_domain_mgr",
        "solution_domain_mgr_gpn": "u_solution_domain_mgr_gpn",
        "swci": "swci_id",
        "swci_id": "swci_id",
        "swc": "swc",
        "swc_id": "swc_id",
        "state": "state",
        "business_impact_severity": "business_impact",
        "resolution_sla_name": "made_sla",
        "response_ola_name": "made_sla",
        "1st_assigned_grp": "assignment_group",
        "updated_by_gpn": "sys_updated_by",
        "resolved_by_name": "resolved_by_user_name",
        "attached_kb_number": "knowledge",
        "related_item": "parent",
        "source_name": "origin_table",
        "service_type": "type",
        "technology_bcm_tier": "u_technology_bcm_tier",
        "bcm_criticality_tier_rating": "u_bcm_criticality_tier_rating",
    }
    return mapping

# ------------------------------------------------------------------------
# Data Fetching Functions
# ------------------------------------------------------------------------
def fetch_atlas_data(session, start_date="2024-01-01", end_date=None):
    """Fetch data from Atlas (IPM Incident Flat) for DH assignment groups"""
    if end_date is None:
        end_date = datetime.datetime.now().strftime("%Y-%m-%d")
    
    print("\n" + "=" * 80)
    print("üì• FETCHING ATLAS DATA")
    print("=" * 80)
    print(f"üìÖ Period: {start_date} to {end_date} (end exclusive)")
    
    catalog = "ipm_service_management"
    schema = "ipm_service_management_schema"
    table = "ipm_incident_flat"
    fqtn = f"{catalog}.{schema}.{table}"
    date_col = "opened_at"
    
    atlas_columns = [
        "incident_number", "opened_at", "opened_by_user_name", "opened_by_gpn",
        "closed_at", "resolved_at", "assignment_group", "assignment_group_id",
        "assigned_to_gpn", "assigned_to_user_name", "caller_gpn", "caller_user_name",
        "state", "priority", "severity", "category", "subcategory", "short_description",
        "description", "impact", "urgency", "business_impact", "business_service",
        "cmdb_ci", "cmdb_ci_sys_id", "location", "company", "active", "incident_state",
        "reassignment_count", "reopen_count", "time_worked", "business_duration",
        "calendar_duration", "made_sla", "u_manager_in_charge_gpn",
        "u_manager_in_charge_user_name", "u_financial_impact", "u_operational_impact",
        "u_reputational_impact", "u_regulatory_impact", "u_regulators_informed",
        "u_impacted_country", "sys_created_on", "sys_updated_on", "sys_updated_by",
        "resolved_by_user_name", "resolved_by_gpn", "close_notes", "knowledge",
        "parent", "origin_table", "contact_type",
    ]
    
    normalized_expr = f"regexp_replace({date_col}, '(.*)([+-]\\\\d{{2}})(\\\\d{{2}})$', '\\\\1\\\\2:\\\\3')"
    parse_iso = f"from_iso8601_timestamp({normalized_expr})"
    
    col_list_sql = ", ".join([f'"{c}"' for c in atlas_columns])
    
    query = f"""
    SELECT {col_list_sql}
    FROM {fqtn}
    WHERE {normalized_expr} IS NOT NULL
      AND {parse_iso} >= TIMESTAMP '{start_date} 00:00:00'
      AND {parse_iso} <  TIMESTAMP '{end_date} 00:00:00'
      AND assignment_group LIKE 'DH-%'
    ORDER BY {parse_iso} DESC
    """
    
    try:
        print(f"‚è≥ Fetching Atlas records...")
        df_atlas = session.sql(query).to_pandas()
        df_atlas['data_source'] = 'Atlas'
        print(f"‚úÖ Retrieved {len(df_atlas):,} Atlas records")
        return df_atlas
    except Exception as e:
        print(f"‚ùå Error fetching Atlas data: {e}")
        traceback.print_exc()
        return None

def fetch_gsnow_data(session, start_date="2024-01-01", end_date=None):
    """Fetch data from gsnow for DH assignment groups"""
    if end_date is None:
        end_date = datetime.datetime.now().strftime("%Y-%m-%d")
    
    print("\n" + "=" * 80)
    print("üì• FETCHING GSNOW DATA")
    print("=" * 80)
    print(f"üìÖ Period: {start_date} to {end_date} (end exclusive)")
    
    schema = "ts_dwh_gsnow_schema"
    table = "gsnow_inc_main_unstrreq_36mth"
    table_ref = f"{schema}.{table}"
    date_col = "opened_at"
    
    gsnow_columns = [
        "inc_number", "assigned_grp", "business_impact", "category", "closed_at",
        "contact_type", "created_on", "opened_at", "operational_impact", "inc_priority",
        "resolution_sla_name", "resolved_at", "response_ola_name", "service_impacted",
        "service_impacted_id", "service_owner_grp", "severity", "short_description",
        "state", "type", "updated_on", "urgency", "creator_bus_name", "creator_gpn",
        "service_environment", "impacted_country", "description", "sub-category",
        "ola_created_by_bus_name", "ola_created_by_gpn", "1st_assigned_grp",
        "updated_by_gpn", "business_impact_severity", "swci", "swci_id", "swc",
        "swc_id", "bcm_criticality_tier_rating", "solution", "solution_id",
        "solution_domain_mgr", "solution_domain_mgr_gpn", "solution_domain",
        "solution_domain_id", "business_cluster_owner", "business_cluster_owner_gpn",
        "business_cluster", "business_cluster_id", "it_sector_cio", "it_sector",
        "business_stream_prg_mgr", "business_stream_prg_mgr_gpn", "business_stream",
        "business_stream_id", "service_division_id", "service_division",
        "attached_kb_number", "related_item", "source_name", "incremental_field",
        "ext_name", "service_type", "technology_bcm_tier", "resolved_by_name",
    ]
    
    col_list_sql = ", ".join([f'"{c}"' for c in gsnow_columns])
    
    query = f"""
    SELECT {col_list_sql}
    FROM {table_ref}
    WHERE {date_col} >= DATE '{start_date}'
      AND {date_col} <  DATE '{end_date}'
      AND assigned_grp LIKE 'DH-%'
    ORDER BY {date_col} DESC
    """
    
    try:
        print(f"‚è≥ Fetching gsnow records...")
        df_gsnow = session.sql(query).to_pandas()
        df_gsnow['data_source'] = 'gsnow'
        print(f"‚úÖ Retrieved {len(df_gsnow):,} gsnow records")
        return df_gsnow
    except Exception as e:
        print(f"‚ùå Error fetching gsnow data: {e}")
        traceback.print_exc()
        return None

# ------------------------------------------------------------------------
# Data Merging and Fixing Functions
# ------------------------------------------------------------------------
def map_and_combine(df_atlas, df_gsnow):
    """Map gsnow columns to Atlas columns and combine datasets"""
    print("\n" + "=" * 80)
    print("üîÑ MAPPING COLUMNS AND COMBINING DATASETS")
    print("=" * 80)
    
    if df_atlas is None and df_gsnow is None:
        print("‚ùå No data from either source")
        return None
    
    if df_atlas is None:
        print("‚ö†Ô∏è Warning: No Atlas data available")
        return None
    
    column_mapping = get_column_mapping()
    atlas_columns = list(df_atlas.columns)
    standard_columns = atlas_columns.copy()
    
    print(f"üìã Atlas (PRIMARY) columns: {len(standard_columns)}")
    print(f"‚úÖ Atlas data: {len(df_atlas):,} records")
    
    # Process gsnow data - map to Atlas structure
    if df_gsnow is not None:
        print(f"‚úÖ gsnow data: {len(df_gsnow):,} records")
        print("üîÑ Mapping gsnow columns to Atlas structure...")
        
        df_gsnow_mapped = pd.DataFrame()
        reverse_mapping = {v: k for k, v in column_mapping.items() if v is not None}
        
        for atlas_col in standard_columns:
            if atlas_col == 'data_source':
                df_gsnow_mapped[atlas_col] = df_gsnow.get('data_source', 'gsnow')
            elif atlas_col in reverse_mapping:
                gsnow_col = reverse_mapping[atlas_col]
                if gsnow_col in df_gsnow.columns:
                    df_gsnow_mapped[atlas_col] = df_gsnow[gsnow_col]
                else:
                    df_gsnow_mapped[atlas_col] = None
            else:
                df_gsnow_mapped[atlas_col] = None
        
        df_gsnow = df_gsnow_mapped
        print(f"‚úÖ gsnow columns mapped to Atlas structure")
    
    # Combine datasets
    print(f"\nüîÄ Combining datasets...")
    dataframes = []
    
    if df_atlas is not None:
        dataframes.append(df_atlas[standard_columns])
        print(f"   üìä Added {len(df_atlas):,} Atlas records (PRIMARY)")
    
    if df_gsnow is not None and len(df_gsnow) > 0:
        dataframes.append(df_gsnow[standard_columns])
        print(f"   üìä Added {len(df_gsnow):,} gsnow records (supplementary)")
    
    if not dataframes:
        print("‚ùå No dataframes to combine")
        return None
    
    df_combined = pd.concat(dataframes, ignore_index=True)
    print(f"‚úÖ Combined dataset: {len(df_combined):,} total records")
    
    # Handle duplicates - prioritize Atlas
    if 'incident_number' in df_combined.columns:
        initial_count = len(df_combined)
        df_combined['_priority'] = df_combined['data_source'].map({'Atlas': 1, 'gsnow': 2})
        df_combined = df_combined.sort_values('_priority').drop_duplicates(
            subset=['incident_number'], 
            keep='first'
        )
        df_combined = df_combined.drop(columns=['_priority'])
        removed = initial_count - len(df_combined)
        if removed > 0:
            print(f"üîç Removed {removed:,} duplicate incident_numbers (kept Atlas records)")
    
    return df_combined

def fix_opened_at_for_gsnow(df_combined):
    """Fix NULL opened_at for gsnow records using sys_created_on as fallback"""
    print("\n" + "=" * 80)
    print("üîß FIXING opened_at FOR GSNOW RECORDS")
    print("=" * 80)
    
    if df_combined is None or len(df_combined) == 0:
        return df_combined
    
    if 'opened_at' not in df_combined.columns or 'sys_created_on' not in df_combined.columns:
        print("‚ö†Ô∏è  Missing required columns (opened_at or sys_created_on)")
        return df_combined
    
    # Find gsnow records with NULL opened_at
    gsnow_mask = (df_combined['data_source'] == 'gsnow') & (df_combined['opened_at'].isna())
    null_count = gsnow_mask.sum()
    
    if null_count > 0:
        print(f"‚ö†Ô∏è  Found {null_count:,} gsnow records with NULL opened_at")
        print("   Using sys_created_on (from created_on) as fallback...")
        
        # Use sys_created_on as fallback for opened_at
        df_combined.loc[gsnow_mask, 'opened_at'] = df_combined.loc[gsnow_mask, 'sys_created_on']
        
        # Count how many were successfully filled
        filled_count = ((df_combined['data_source'] == 'gsnow') & 
                       (df_combined['opened_at'].notna()) & 
                       gsnow_mask).sum()
        
        print(f"‚úÖ Filled {filled_count:,} NULL opened_at values with sys_created_on")
        
        # Count remaining NULLs
        remaining_nulls = ((df_combined['data_source'] == 'gsnow') & 
                          (df_combined['opened_at'].isna())).sum()
        if remaining_nulls > 0:
            print(f"‚ö†Ô∏è  {remaining_nulls:,} gsnow records still have NULL opened_at (sys_created_on also NULL)")
    else:
        print("‚úÖ No gsnow records with NULL opened_at found")
    
    return df_combined

# ------------------------------------------------------------------------
# Categorization Function
# ------------------------------------------------------------------------
def categorize_with_category_latest(input_csv, output_csv=None):
    """Run category_latest.py to categorize the data"""
    print("\n" + "=" * 80)
    print("üîç RUNNING CATEGORIZATION (category_latest.py)")
    print("=" * 80)
    
    # Find category_latest.py script
    script_dir = Path(__file__).parent.resolve()
    category_script = None
    
    for folder_name in ["Categorise", "categorization", "Categorization"]:
        potential_path = script_dir / folder_name / "category_latest.py"
        if potential_path.exists():
            category_script = potential_path
            print(f"‚úÖ Found category_latest.py at: {category_script}")
            break
    
    if not category_script:
        print(f"‚ùå category_latest.py not found")
        return None
    
    # Use provided output or create temp file
    if output_csv is None:
        output_csv = input_csv.replace('.csv', '_categorized.csv')
    
    # Build command
    cmd = [
        sys.executable,
        str(category_script),
        '-i', input_csv,
        '-o', output_csv,
        '--use-gpt',
        '--cluster-categories',
        '--cluster-method', 'gpt',
        '--skip-drain3',
        '--max-workers', '50',
    ]
    
    print(f"üöÄ Calling category_latest.py...")
    print(f"   Input: {input_csv}")
    print(f"   Output: {output_csv}")
    
    try:
        start_time = datetime.datetime.now()
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()
        
        if result.returncode != 0:
            print(f"‚ùå category_latest.py failed with return code {result.returncode}")
            if result.stderr:
                print("STDERR:")
                print(result.stderr[-2000:])  # Last 2000 chars
            if result.stdout:
                print("STDOUT (last 2000 chars):")
                print(result.stdout[-2000:])
            return None
        
        print(f"‚úÖ category_latest.py completed in {elapsed_time:.1f}s")
        
        # Load categorized CSV
        if os.path.exists(output_csv):
            print(f"üì• Loading categorized data from: {output_csv}")
            df_categorized = pd.read_csv(output_csv, encoding='utf-8', low_memory=False)
            print(f"‚úÖ Loaded categorized data: {len(df_categorized):,} rows")
            return df_categorized
        else:
            print(f"‚ùå Output file not found: {output_csv}")
            return None
            
    except Exception as e:
        print(f"‚ùå Error during categorization: {e}")
        traceback.print_exc()
        return None

# ------------------------------------------------------------------------
# PostgreSQL Functions
# ------------------------------------------------------------------------
def infer_postgres_type(series, col_name):
    """Infer PostgreSQL data type from pandas Series"""
    non_null = series.dropna()
    
    if len(non_null) == 0:
        return Text
    
    if series.dtype == 'bool' or non_null.dtype == 'bool':
        return Boolean
    
    if pd.api.types.is_datetime64_any_dtype(series):
        return DateTime
    
    if pd.api.types.is_integer_dtype(series):
        if non_null.abs().max() < 2147483647:
            return Integer
        else:
            return BigInteger
    
    if pd.api.types.is_float_dtype(series):
        return Float
    
    if series.dtype == 'object':
        max_length = non_null.astype(str).str.len().max()
        if max_length > 0 and max_length < 255:
            return String(max_length + 50)
        else:
            return Text
    
    return Text

def create_table_from_dataframe(engine, df, table_name, schema='public'):
    """Create PostgreSQL table from DataFrame with inferred data types"""
    try:
        print(f"\nüìã CREATING TABLE: {table_name}")
        
        column_types = {}
        for col in df.columns:
            pg_type = infer_postgres_type(df[col], col)
            column_types[col] = pg_type
        
        columns_sql = []
        for col in df.columns:
            pg_type = column_types[col]
            
            if pg_type == Boolean:
                type_str = "BOOLEAN"
            elif pg_type == Integer:
                type_str = "INTEGER"
            elif pg_type == BigInteger:
                type_str = "BIGINT"
            elif pg_type == Float:
                type_str = "DOUBLE PRECISION"
            elif pg_type == DateTime:
                type_str = "TIMESTAMP"
            elif pg_type == Date:
                type_str = "DATE"
            elif isinstance(pg_type, String):
                type_str = f"VARCHAR({pg_type.length})"
            else:
                type_str = "TEXT"
            
            col_escaped = f'"{col}"'
            columns_sql.append(f"{col_escaped} {type_str}")
        
        # Add created_at and updated_at
        columns_sql.append('"created_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        columns_sql.append('"updated_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {schema}."{table_name}" (
            {', '.join(columns_sql)}
        );
        """
        
        with engine.connect() as conn:
            conn.execute(text(create_table_sql))
            conn.commit()
        
        print(f"‚úÖ Table '{table_name}' created successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Error creating table: {e}")
        traceback.print_exc()
        return False

def load_to_postgresql(df, table_name='incident_data', database='service_automation_db', truncate=True):
    """Load DataFrame to PostgreSQL table"""
    if not POSTGRES_AVAILABLE:
        print("‚ùå PostgreSQL libraries not available")
        return False
    
    if df is None or len(df) == 0:
        print("‚ùå No data to load")
        return False
    
    print("\n" + "=" * 80)
    print("üíæ LOADING DATA TO POSTGRESQL")
    print("=" * 80)
    
    pg_host = POSTGRES_CONFIG['host']
    pg_port = POSTGRES_CONFIG['port']
    pg_db = database or POSTGRES_CONFIG['database']
    pg_user = POSTGRES_CONFIG['user']
    pg_password = POSTGRES_CONFIG['password']
    
    try:
        encoded_password = quote_plus(pg_password)
        connection_string = f"postgresql://{pg_user}:{encoded_password}@{pg_host}:{pg_port}/{pg_db}"
        
        print(f"üìä Database: {pg_db}")
        print(f"üìã Table: {table_name}")
        print(f"üìà Rows to insert: {len(df):,}")
        
        engine = create_engine(connection_string, pool_pre_ping=True)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("‚úÖ Connected to PostgreSQL")
        
        # Check if table exists (case-sensitive check)
        with engine.connect() as conn:
            result = conn.execute(text(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """))
            table_exists = result.fetchone()[0]
        
        if not table_exists:
            print(f"‚ö†Ô∏è  Table '{table_name}' does not exist. Creating it...")
            success = create_table_from_dataframe(engine, df, table_name)
            if not success:
                engine.dispose()
                return False
        else:
            print(f"‚úÖ Table '{table_name}' exists")
        
        # Truncate table FIRST (clean the table)
        if truncate:
            print(f"\nüßπ CLEANING TABLE: Truncating '{table_name}'...")
            with engine.connect() as conn:
                conn.execute(text(f'TRUNCATE TABLE "{table_name}" CASCADE;'))
                conn.commit()
            print(f"‚úÖ Table '{table_name}' cleaned (truncated)")
        
        # Get table columns after truncate
        inspector = inspect(engine)
        table_columns = [col['name'] for col in inspector.get_columns(table_name, schema='public')]
        
        # Prepare DataFrame - match column names
        df_to_insert = df.copy()
        table_columns_lower = {col.lower(): col for col in table_columns}
        
        column_mapping = {}
        for df_col in df_to_insert.columns:
            df_col_lower = df_col.lower()
            if df_col_lower in table_columns_lower:
                table_col = table_columns_lower[df_col_lower]
                if df_col != table_col:
                    column_mapping[df_col] = table_col
        
        if column_mapping:
            df_to_insert.rename(columns=column_mapping, inplace=True)
        
        # Filter to only table columns (excluding metadata)
        metadata_cols = {'created_at', 'updated_at'}
        columns_to_insert = [col for col in table_columns if col not in metadata_cols]
        df_to_insert = df_to_insert[[col for col in columns_to_insert if col in df_to_insert.columns]].copy()
        
        # Insert data with progress bar
        print(f"\n‚è≥ Inserting {len(df_to_insert):,} rows...")
        total_rows = len(df_to_insert)
        chunksize = 1000
        
        try:
            from tqdm import tqdm
            with tqdm(total=total_rows, desc="Inserting", unit="row", unit_scale=True) as pbar:
                for i in range(0, total_rows, chunksize):
                    chunk_df = df_to_insert.iloc[i:i+chunksize]
                    chunk_df.to_sql(
                        name=table_name,
                        con=engine,
                        schema='public',
                        if_exists='append',
                        index=False,
                        method='multi',
                        chunksize=chunksize
                    )
                    pbar.update(len(chunk_df))
        except ImportError:
            # Fallback without progress bar
            df_to_insert.to_sql(
                name=table_name,
                con=engine,
                schema='public',
                if_exists='append',
                index=False,
                method='multi',
                chunksize=chunksize
            )
        
        print(f"‚úÖ Successfully inserted {len(df_to_insert):,} rows")
        
        # Verify
        with engine.connect() as conn:
            result = conn.execute(text(f'SELECT COUNT(*) FROM "{table_name}"'))
            count = result.fetchone()[0]
            print(f"‚úÖ Verified: {count:,} rows in table")
        
        engine.dispose()
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading to PostgreSQL: {e}")
        traceback.print_exc()
        return False

# ------------------------------------------------------------------------
# Main Pipeline Function
# ------------------------------------------------------------------------
def run_complete_pipeline(start_date="2024-01-01", end_date=None, 
                         output_csv=None, table_name='Incident_Data',
                         truncate_table=True, skip_categorization=False):
    """
    Complete pipeline: Fetch -> Merge -> Fix -> Export -> Categorize -> Load to DB
    """
    print("=" * 80)
    print("üöÄ COMPLETE INCIDENT DATA PIPELINE")
    print("=" * 80)
    print(f"üìÖ Date range: {start_date} to {end_date or 'current'}")
    print(f"üìä Output CSV: {output_csv or 'auto-generated'}")
    print(f"üíæ PostgreSQL table: {table_name}")
    print("=" * 80)
    
    try:
        # Step 1: Connect to databases
        print("\n" + "=" * 80)
        print("STEP 1: CONNECTING TO DATABASES")
        print("=" * 80)
        
        print("\nüì° Connecting to Atlas...")
        atlas_session, atlas_catalog = create_atlas_session()
        if not atlas_session:
            print("‚ùå Failed to connect to Atlas")
            return False
        print(f"‚úÖ Connected to Atlas: {atlas_catalog}")
        
        print("\nüì° Connecting to gsnow...")
        gsnow_session, gsnow_catalog = create_gsnow_session()
        if not gsnow_session:
            print("‚ùå Failed to connect to gsnow")
            return False
        print(f"‚úÖ Connected to gsnow: {gsnow_catalog}")
        
        # Step 2: Fetch data
        print("\n" + "=" * 80)
        print("STEP 2: FETCHING DATA FROM ATLAS AND GSNOW")
        print("=" * 80)
        
        df_atlas = fetch_atlas_data(atlas_session, start_date, end_date)
        df_gsnow = fetch_gsnow_data(gsnow_session, start_date, end_date)
        
        # Step 3: Merge and combine
        print("\n" + "=" * 80)
        print("STEP 3: MERGING DATASETS")
        print("=" * 80)
        
        df_combined = map_and_combine(df_atlas, df_gsnow)
        if df_combined is None:
            print("‚ùå Failed to combine data")
            return False
        
        # Step 4: Fix opened_at for gsnow records
        print("\n" + "=" * 80)
        print("STEP 4: FIXING opened_at FOR GSNOW RECORDS")
        print("=" * 80)
        
        df_combined = fix_opened_at_for_gsnow(df_combined)
        
        # Step 5: Export to CSV
        print("\n" + "=" * 80)
        print("STEP 5: EXPORTING TO CSV")
        print("=" * 80)
        
        if output_csv is None:
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            output_csv = f"incidents_merged_{timestamp}.csv"
        
        print(f"üíæ Saving to: {output_csv}")
        df_combined.to_csv(output_csv, index=False, encoding='utf-8')
        print(f"‚úÖ Exported {len(df_combined):,} rows to {output_csv}")
        
        # Step 6: Categorize (if not skipped)
        df_categorized = None
        categorized_csv = None
        
        if not skip_categorization:
            categorized_csv = output_csv.replace('.csv', '_categorized.csv')
            df_categorized = categorize_with_category_latest(output_csv, categorized_csv)
            
            if df_categorized is None:
                print("‚ö†Ô∏è  Categorization failed, using original data")
                df_categorized = df_combined
                categorized_csv = output_csv
        else:
            print("\n‚è≠Ô∏è  Skipping categorization (--skip-categorization)")
            df_categorized = df_combined
            categorized_csv = output_csv
        
        # Step 7: Load to PostgreSQL
        print("\n" + "=" * 80)
        print("STEP 7: LOADING TO POSTGRESQL")
        print("=" * 80)
        
        success = load_to_postgresql(
            df_categorized,
            table_name=table_name,
            truncate=truncate_table
        )
        
        if success:
            print("\n" + "=" * 80)
            print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY")
            print("=" * 80)
            print(f"üìä Total records processed: {len(df_categorized):,}")
            print(f"üìÅ Merged CSV: {output_csv}")
            if categorized_csv != output_csv:
                print(f"üìÅ Categorized CSV: {categorized_csv}")
            print(f"üíæ PostgreSQL table: {table_name}")
            print("=" * 80)
            return True
        else:
            print("\n‚ùå Pipeline failed at PostgreSQL loading step")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        traceback.print_exc()
        return False

def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Complete Incident Data Pipeline: Fetch -> Merge -> Fix -> Categorize -> Load to DB',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Flow:
1. Pull data from Atlas and gsnow (2024 to current)
2. Merge them properly (Atlas PRIMARY, gsnow supplementary)
3. Fix opened_at issue (use created_on as fallback for gsnow)
4. Export to CSV
5. Run categorization script (category_latest.py) with GPT clustering
6. Load categorized data to PostgreSQL

Examples:
  # Full pipeline with default settings
  python complete_incident_pipeline.py
  
  # Custom date range
  python complete_incident_pipeline.py --start-date 2024-06-01 --end-date 2024-12-31
  
  # Skip categorization
  python complete_incident_pipeline.py --skip-categorization
  
  # Custom output file and table
  python complete_incident_pipeline.py --output my_data.csv --table my_table
        """
    )
    
    parser.add_argument('--start-date', type=str, default='2024-01-01',
                       help='Start date (YYYY-MM-DD, default: 2024-01-01)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='End date (YYYY-MM-DD, default: current date)')
    parser.add_argument('--output', type=str, default=None,
                       help='Output CSV file path (default: auto-generated)')
    parser.add_argument('--table', type=str, default='Incident_Data',
                       help='PostgreSQL table name (default: Incident_Data)')
    parser.add_argument('--no-truncate', action='store_true',
                       help='Do not truncate table before inserting (default: truncate)')
    parser.add_argument('--skip-categorization', action='store_true',
                       help='Skip categorization step (default: run categorization)')
    
    args = parser.parse_args()
    
    success = run_complete_pipeline(
        start_date=args.start_date,
        end_date=args.end_date,
        output_csv=args.output,
        table_name=args.table,
        truncate_table=not args.no_truncate,
        skip_categorization=args.skip_categorization
    )
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()
