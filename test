#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Push categorized incident CSV file to PostgreSQL table Incident_Data
Usage: python push_incidents_to_postgres.py <csv_file_path> [--truncate]
"""

import os
import sys
import pandas as pd
import traceback
import logging
import re
from datetime import datetime
import argparse
from urllib.parse import quote_plus

# Progress bar
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    # Fallback: create a dummy tqdm that does nothing
    class tqdm:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def update(self, n):
            pass
        def set_postfix(self, *args, **kwargs):
            pass

# PostgreSQL client
try:
    import psycopg2
    from sqlalchemy import create_engine, text, inspect
    from sqlalchemy.types import Integer, BigInteger, Float, Boolean, DateTime, Text, String, Date
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("ERROR: PostgreSQL libraries not available.")
    print("Please install: pip install psycopg2-binary sqlalchemy pandas")
    sys.exit(1)

# ------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------
def setup_logging():
    """Set up logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

# ------------------------------------------------------------------------
# PostgreSQL Configuration
# ------------------------------------------------------------------------
POSTGRES_CONFIG = {
    'host': os.getenv('PGHOST', 'xd1e1159676dev.ubscloud-prod.msad.ubs.net'),
    'port': os.getenv('PGPORT', '5432'),
    'database': os.getenv('PGDATABASE', 'service_automation_db'),
    'user': os.getenv('PGUSER', 'powerbi_user'),
    'password': os.getenv('PGPASSWORD', 'Report@123'),
    'table_name': 'incident_data'  # Changed to lowercase incident_data
}

# ------------------------------------------------------------------------
# Data Type Inference
# ------------------------------------------------------------------------
def infer_postgres_type(series, col_name):
    """
    Infer PostgreSQL data type from pandas Series.
    
    Parameters
    ----------
    series : pd.Series
        The pandas Series to analyze
    col_name : str
        Column name (for special handling)
    
    Returns
    -------
    sqlalchemy.types.TypeEngine
        SQLAlchemy type for PostgreSQL
    """
    # Remove null values for type inference
    non_null = series.dropna()
    
    if len(non_null) == 0:
        # All nulls - default to Text
        return Text
    
    # Check for boolean
    if series.dtype == 'bool' or non_null.dtype == 'bool':
        return Boolean
    
    # Check for datetime
    if pd.api.types.is_datetime64_any_dtype(series):
        return DateTime
    
    # Check for date
    if pd.api.types.is_datetime64_any_dtype(series) and 'date' in col_name.lower():
        # Try to parse as date if it's a date column
        try:
            pd.to_datetime(non_null.iloc[0])
            return Date
        except:
            pass
    
    # Check for integer
    if pd.api.types.is_integer_dtype(series):
        # Check if values fit in regular integer or need bigint
        if non_null.abs().max() < 2147483647:  # PostgreSQL INTEGER max
            return Integer
        else:
            return BigInteger
    
    # Check for float
    if pd.api.types.is_float_dtype(series):
        return Float
    
    # Check if numeric string can be converted to int
    if series.dtype == 'object':
        # Try to convert to numeric
        try:
            numeric_series = pd.to_numeric(non_null, errors='coerce')
            if numeric_series.notna().sum() == len(non_null):
                # All values are numeric
                if numeric_series.abs().max() < 2147483647:
                    return Integer
                else:
                    return BigInteger
        except:
            pass
    
    # Check string length for VARCHAR vs TEXT
    if series.dtype == 'object':
        max_length = non_null.astype(str).str.len().max()
        if max_length > 0 and max_length < 255:
            # Use VARCHAR with some buffer
            return String(max_length + 50)  # Add 50 char buffer
        else:
            return Text
    
    # Default to Text
    return Text

def create_table_from_dataframe(engine, df, table_name, schema='public'):
    """
    Create PostgreSQL table from DataFrame with inferred data types.
    
    Parameters
    ----------
    engine : sqlalchemy.engine.Engine
        SQLAlchemy engine
    df : pd.DataFrame
        DataFrame to create table from
    table_name : str
        Table name (will be created in lowercase)
    schema : str
        Schema name (default: 'public')
    
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    try:
        logging.info(f"\n{'='*80}")
        logging.info(f"üìã CREATING TABLE: {table_name}")
        logging.info(f"{'='*80}")
        
        # Infer column types
        column_types = {}
        for col in df.columns:
            pg_type = infer_postgres_type(df[col], col)
            column_types[col] = pg_type
            logging.info(f"   {col}: {pg_type}")
        
        # Build CREATE TABLE SQL
        columns_sql = []
        for col in df.columns:
            pg_type = column_types[col]
            
            # Convert SQLAlchemy type to PostgreSQL type string
            if pg_type == Boolean:
                type_str = "BOOLEAN"
            elif pg_type == Integer:
                type_str = "INTEGER"
            elif pg_type == BigInteger:
                type_str = "BIGINT"
            elif pg_type == Float:
                type_str = "DOUBLE PRECISION"
            elif pg_type == DateTime:
                type_str = "TIMESTAMP"
            elif pg_type == Date:
                type_str = "DATE"
            elif isinstance(pg_type, String):
                type_str = f"VARCHAR({pg_type.length})"
            else:
                type_str = "TEXT"
            
            # Escape column name if needed
            col_escaped = f'"{col}"'
            columns_sql.append(f"{col_escaped} {type_str}")
        
        # Add created_at and updated_at timestamps
        columns_sql.append('"created_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        columns_sql.append('"updated_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {schema}."{table_name}" (
            {', '.join(columns_sql)}
        );
        """
        
        logging.info(f"\n‚è≥ Creating table '{table_name}'...")
        with engine.connect() as conn:
            conn.execute(text(create_table_sql))
            conn.commit()
        
        logging.info(f"‚úÖ Table '{table_name}' created successfully")
        logging.info(f"   Columns: {len(df.columns) + 2} (including created_at, updated_at)")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error creating table: {e}")
        logging.error(traceback.format_exc())
        return False

# ------------------------------------------------------------------------
# Main Function
# ------------------------------------------------------------------------
def push_incidents_to_postgresql(csv_file_path, table_name=None, database=None, 
                                 user=None, host=None, port=None, password=None,
                                 truncate=False, create_table=True):
    """
    Push categorized incident CSV file to PostgreSQL table.
    
    Parameters
    ----------
    csv_file_path : str
        Path to the categorized CSV file to import
    table_name : str, optional
        Target table name (default: 'Incident_Data')
    database : str, optional
        Database name (default: from config or 'service_automation_db')
    user : str, optional
        PostgreSQL username (default: from config)
    host : str, optional
        PostgreSQL host (default: from config)
    port : str, optional
        PostgreSQL port (default: from config or '5432')
    password : str, optional
        PostgreSQL password (default: from environment variable PGPASSWORD)
    truncate : bool, optional
        If True, truncate table before inserting (default: False)
        
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    if not POSTGRES_AVAILABLE:
        logging.error("‚úó PostgreSQL libraries not available. Cannot export to PostgreSQL.")
        return False
    
    if not os.path.exists(csv_file_path):
        logging.error(f"‚úó CSV file not found: {csv_file_path}")
        return False
    
    # Use provided values or fall back to config/environment
    table_name = (table_name or POSTGRES_CONFIG['table_name']).lower()  # Ensure lowercase
    database = database or POSTGRES_CONFIG['database']
    user = user or POSTGRES_CONFIG['user']
    host = host or POSTGRES_CONFIG['host']
    port = port or POSTGRES_CONFIG['port']
    password = password or POSTGRES_CONFIG['password']
    
    try:
        logging.info("="*80)
        logging.info("üì§ Pushing Incident CSV to PostgreSQL")
        logging.info("="*80)
        logging.info(f"   CSV file: {csv_file_path}")
        logging.info(f"   File size: {os.path.getsize(csv_file_path) / (1024*1024):.2f} MB" if os.path.exists(csv_file_path) else "   File size: Unknown")
        logging.info(f"   Database: {database}")
        logging.info(f"   Table: {table_name}")
        logging.info(f"   Host: {host}:{port}")
        logging.info(f"   User: {user}")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info(f"   Create table: {create_table}")
        logging.info(f"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info("="*80)
        
        # Read CSV file
        logging.info(f"\n‚è≥ Reading CSV file...")
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8', low_memory=False)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_file_path, encoding='latin-1', low_memory=False)
            except:
                df = pd.read_csv(csv_file_path, encoding='cp1252', low_memory=False)
        
        logging.info(f"‚úì Read {len(df):,} rows from CSV")
        logging.info(f"‚úì Columns: {len(df.columns)} ({', '.join(df.columns[:5])}...)")
        
        # Convert GPN columns from float64 to string (preserve values, handle NaN)
        logging.info(f"\n‚è≥ Converting GPN columns from float64 to string...")
        gpn_columns = ['resolved_by_gpn', 'assigned_to_gpn', 'caller_gpn', 'u_manager_in_charge_gpn']
        for col in gpn_columns:
            if col in df.columns:
                try:
                    initial_dtype = df[col].dtype
                    non_null_before = df[col].notna().sum()
                    
                    if df[col].dtype == 'float64':
                        # Convert float to string, handling NaN
                        df[col] = df[col].astype('Int64').astype(str).replace('<NA>', None).replace('nan', None)
                        non_null_after = df[col].notna().sum()
                        logging.info(f"   ‚úì Converted {col} from {initial_dtype} to string (non-null: {non_null_before} -> {non_null_after})")
                    elif df[col].dtype == 'object':
                        # Already string, but ensure NaN handling
                        df[col] = df[col].astype(str).replace('nan', None).replace('None', None)
                        logging.info(f"   ‚úì Cleaned {col} (already string, handled NaN)")
                    else:
                        logging.warning(f"   ‚ö†Ô∏è  {col} has unexpected dtype: {initial_dtype}")
                except Exception as e:
                    logging.error(f"   ‚ùå Error converting {col}: {e}")
                    logging.error(traceback.format_exc())
        
        # Custom date parsing function to handle various formats
        def parse_datetime_robust(series):
            """Parse datetime with handling for various malformed formats"""
            if series.dtype != 'object':
                return pd.to_datetime(series, errors='coerce')
            
            # Convert to string and handle NaN
            str_series = series.astype(str)
            str_series = str_series.replace('nan', None)
            str_series = str_series.replace('None', None)
            str_series = str_series.replace('', None)
            
            # Normalize common malformed formats
            def normalize_date(date_str):
                if pd.isna(date_str) or date_str is None or date_str == 'None' or date_str == '':
                    return None
                
                date_str = str(date_str).strip()
                
                # Handle format: 2026-01-1407:13:58.001+0000 (missing space)
                # Pattern: YYYY-MM-DDHH:MM:SS.mmm+TZ
                if re.match(r'^\d{4}-\d{2}-\d{2}\d{2}:\d{2}:\d{2}', date_str):
                    # Insert space between date and time
                    date_str = re.sub(r'^(\d{4}-\d{2}-\d{2})(\d{2}:\d{2}:\d{2})', r'\1 \2', date_str)
                
                # Handle format: 2026-01-14 07:13:58:57+00:00 (colon instead of dot for milliseconds)
                # Pattern: YYYY-MM-DD HH:MM:SS:mmm+TZ or YYYY-MM-DD HH:MM:SS:mmm+TZ:ZZ
                # Replace colon before timezone (but not the timezone colon) with dot
                # Match pattern: HH:MM:SS:mmm followed by timezone
                if re.search(r'\d{2}:\d{2}:\d{2}:\d{1,3}[+-]', date_str):
                    # Replace the colon between seconds and milliseconds (before timezone)
                    date_str = re.sub(r'(\d{2}:\d{2}:\d{2}):(\d{1,3})([+-])', r'\1.\2\3', date_str)
                
                # Normalize timezone format: +0000 to +00:00
                if re.search(r'[+-]\d{4}$', date_str):
                    # Format: +0000 or -0500 at the end
                    date_str = re.sub(r'([+-])(\d{2})(\d{2})$', r'\1\2:\3', date_str)
                
                return date_str
            
            # Apply normalization
            normalized = str_series.apply(normalize_date)
            
            # Try parsing with multiple formats
            try:
                # First try with pandas default parser
                result = pd.to_datetime(normalized, errors='coerce', infer_datetime_format=True)
                
                # Check if we have any nulls that might be parseable with different format
                null_mask = result.isna()
                if null_mask.any():
                    # Try alternative parsing for null values
                    for fmt in [
                        '%Y-%m-%d %H:%M:%S.%f%z',
                        '%Y-%m-%d %H:%M:%S%z',
                        '%Y-%m-%d %H:%M:%S',
                        '%Y-%m-%dT%H:%M:%S.%f%z',
                        '%Y-%m-%dT%H:%M:%S%z',
                    ]:
                        try:
                            alt_result = pd.to_datetime(normalized[null_mask], format=fmt, errors='coerce')
                            result.loc[null_mask] = alt_result
                            null_mask = result.isna()
                            if not null_mask.any():
                                break
                        except:
                            continue
                
                return result
            except Exception as e:
                logging.warning(f"   ‚ö†Ô∏è  Date parsing error: {e}, using fallback")
                return pd.to_datetime(normalized, errors='coerce')
        
        # Convert date columns to datetime if they're strings
        # Exclude duration columns and user/GPN columns from date parsing
        exclude_from_dates = ['business_duration', 'calendar_duration', 'time_worked', 
                             'opened_by_user_name', 'opened_by_gpn', 'assigned_to_user_name', 
                             'assigned_to_gpn', 'caller_user_name', 'caller_gpn', 
                             'resolved_by_user_name', 'resolved_by_gpn', 'sys_updated_by',
                             'u_manager_in_charge_user_name', 'u_manager_in_charge_gpn']
        
        logging.info(f"\n‚è≥ Converting date columns to datetime...")
        logging.info(f"   Excluding from date parsing: {len(exclude_from_dates)} columns")
        
        date_columns = ['opened_at', 'closed_at', 'resolved_at', 'sys_created_on', 'sys_updated_on', 
                        'created_on', 'updated_on', 'opened_date', 'closed_date']
        date_conversion_stats = {'success': 0, 'partial': 0, 'failed': 0, 'skipped': 0}
        
        for col in df.columns:
            # Skip excluded columns
            if col in exclude_from_dates:
                date_conversion_stats['skipped'] += 1
                logging.debug(f"   ‚è≠Ô∏è  Skipped {col} (excluded from date parsing)")
                continue
            # Only process date-like columns
            if any(date_keyword in col.lower() for date_keyword in ['date', 'time', 'created', 'updated', 'opened', 'closed', 'resolved']):
                if df[col].dtype == 'object':
                    try:
                        initial_non_null = df[col].notna().sum()
                        df[col] = parse_datetime_robust(df[col])
                        # Check how many were successfully parsed
                        parsed_count = df[col].notna().sum()
                        total_count = len(df[col])
                        
                        if parsed_count == 0 and initial_non_null > 0:
                            date_conversion_stats['failed'] += 1
                            logging.error(f"   ‚ùå {col}: Failed to parse any values (had {initial_non_null:,} non-null)")
                            # Try basic conversion as fallback
                            try:
                                df[col] = pd.to_datetime(df[col], errors='coerce')
                                retry_parsed = df[col].notna().sum()
                                if retry_parsed > 0:
                                    logging.info(f"      ‚úì Fallback parsing succeeded: {retry_parsed:,} values")
                            except Exception as fallback_error:
                                logging.error(f"      ‚ùå Fallback parsing also failed: {fallback_error}")
                        elif parsed_count < total_count and initial_non_null > 0:
                            failed_count = total_count - parsed_count
                            date_conversion_stats['partial'] += 1
                            logging.warning(f"   ‚ö†Ô∏è  {col}: {parsed_count:,}/{total_count:,} values parsed successfully ({failed_count:,} failed)")
                        elif parsed_count > 0:
                            date_conversion_stats['success'] += 1
                            logging.info(f"   ‚úì Converted {col} to datetime ({parsed_count:,} values)")
                        else:
                            date_conversion_stats['skipped'] += 1
                            logging.debug(f"   ‚è≠Ô∏è  {col}: All null, skipped")
                    except Exception as e:
                        date_conversion_stats['failed'] += 1
                        logging.error(f"   ‚ùå Error converting {col} to datetime: {e}")
                        logging.error(f"      Exception type: {type(e).__name__}")
                        logging.error(traceback.format_exc())
                        # Try basic conversion as fallback
                        try:
                            df[col] = pd.to_datetime(df[col], errors='coerce')
                            logging.info(f"      ‚úì Fallback conversion attempted")
                        except Exception as fallback_error:
                            logging.error(f"      ‚ùå Fallback conversion failed: {fallback_error}")
        
        logging.info(f"   Date conversion summary: {date_conversion_stats['success']} successful, "
                    f"{date_conversion_stats['partial']} partial, {date_conversion_stats['failed']} failed, "
                    f"{date_conversion_stats['skipped']} skipped")
        
        # Convert boolean columns (handle BIT fields)
        logging.info(f"\n‚è≥ Converting boolean columns...")
        bool_columns = ['active', 'made_sla', 'knowledge']
        for col in bool_columns:
            if col in df.columns:
                try:
                    initial_dtype = df[col].dtype
                    initial_non_null = df[col].notna().sum()
                    
                    if df[col].dtype == 'object':
                        # Check if it's already boolean-like
                        non_null = df[col].dropna()
                        if len(non_null) > 0:
                            unique_vals = non_null.astype(str).str.lower().unique()
                            # Check if values are boolean-like
                            bool_like_values = ['true', 'false', '1', '0', 'yes', 'no', 't', 'f', 'y', 'n', 'on', 'off']
                            bool_like = all(v in bool_like_values for v in unique_vals)
                            
                            if bool_like and len(unique_vals) <= 4:
                                # Convert to boolean
                                df[col] = df[col].astype(str).str.lower().isin(['true', '1', 'yes', 'y', 't', 'on'])
                                final_non_null = df[col].notna().sum()
                                logging.info(f"   ‚úì Converted {col} to boolean (non-null: {initial_non_null} -> {final_non_null}, unique values: {list(unique_vals)})")
                            else:
                                # Keep as string but log
                                logging.warning(f"   ‚ö†Ô∏è  {col} kept as string (not boolean-like: {list(unique_vals[:5])})")
                        else:
                            # All null, set to boolean with all False
                            df[col] = False
                            logging.info(f"   ‚úì Set {col} to boolean (all False, was all null)")
                    else:
                        df[col] = df[col].astype(bool)
                        final_non_null = df[col].notna().sum()
                        logging.info(f"   ‚úì Converted {col} from {initial_dtype} to boolean (non-null: {initial_non_null} -> {final_non_null})")
                except Exception as e:
                    logging.error(f"   ‚ùå Error converting {col} to boolean: {e}")
                    logging.error(traceback.format_exc())
        
        # Convert integer columns
        logging.info(f"\n‚è≥ Converting integer columns...")
        int_columns = ['reassignment_count', 'reopen_count']
        for col in int_columns:
            if col in df.columns:
                try:
                    initial_dtype = df[col].dtype
                    initial_non_null = df[col].notna().sum()
                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
                    final_non_null = df[col].notna().sum()
                    logging.info(f"   ‚úì Converted {col} from {initial_dtype} to Int64 (non-null: {initial_non_null} -> {final_non_null})")
                except Exception as e:
                    logging.error(f"   ‚ùå Error converting {col} to integer: {e}")
                    logging.error(traceback.format_exc())
        
        # Keep duration fields as VARCHAR (don't convert to numeric or dates)
        # These should remain as strings per schema: time_worked, business_duration, calendar_duration
        logging.info(f"\n‚è≥ Ensuring duration columns are treated as strings...")
        duration_columns = ['time_worked', 'business_duration', 'calendar_duration']
        for col in duration_columns:
            if col in df.columns:
                try:
                    initial_dtype = df[col].dtype
                    initial_non_null = df[col].notna().sum()
                    
                    # Convert to string, handling all data types
                    if df[col].dtype == 'float64':
                        # Convert float to string, handling NaN
                        df[col] = df[col].astype('Int64').astype(str).replace('<NA>', None).replace('nan', None)
                    else:
                        df[col] = df[col].astype(str).replace('nan', None).replace('None', None)
                    
                    final_non_null = df[col].notna().sum()
                    logging.info(f"   ‚úì Ensured {col} is treated as string (dtype: {initial_dtype} -> object, non-null: {initial_non_null} -> {final_non_null})")
                except Exception as e:
                    logging.error(f"   ‚ùå Error converting {col} to string: {e}")
                    logging.error(traceback.format_exc())
        
        # Create connection string with URL-encoded password
        # Fix: Use quote_plus to properly encode password (handles @, #, etc.)
        if password:
            # URL-encode the password to handle special characters like @
            encoded_password = quote_plus(password)
            connection_string = f"postgresql://{user}:{encoded_password}@{host}:{port}/{database}"
        else:
            # Try without password (trust authentication or .pgpass)
            connection_string = f"postgresql://{user}@{host}:{port}/{database}"
        
        # Create SQLAlchemy engine
        logging.info(f"\n‚è≥ Connecting to PostgreSQL...")
        engine = create_engine(connection_string, pool_pre_ping=True)
        
        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logging.info(f"‚úì Connected to PostgreSQL successfully")
        
        # Check if table exists
        logging.info(f"\n‚è≥ Checking if table '{table_name}' exists...")
        inspector = inspect(engine)
        table_exists = inspector.has_table(table_name, schema='public')
        
        if not table_exists:
            if create_table:
                logging.info(f"‚ö†Ô∏è  Table '{table_name}' does not exist. Creating it...")
                success = create_table_from_dataframe(engine, df, table_name)
                if not success:
                    logging.error(f"‚úó Failed to create table '{table_name}'")
                    engine.dispose()
                    return False
            else:
                logging.error(f"‚úó Table '{table_name}' does not exist in database '{database}'")
                logging.error(f"  Use --create-table to create it automatically")
                engine.dispose()
                return False
        else:
            logging.info(f"‚úì Table '{table_name}' exists")
        
        # Get table column names
        logging.info(f"\n‚è≥ Fetching table column names...")
        table_columns = [col['name'] for col in inspector.get_columns(table_name, schema='public')]
        
        col_count = len(table_columns)
        logging.info(f"‚úì Table has {col_count} columns")
        
        # Validate CSV columns match table columns
        logging.info(f"\n‚è≥ Validating column names...")
        
        csv_columns = [col for col in df.columns]
        table_columns_lower = {col.lower(): col for col in table_columns}
        
        # Check for resolved_by_user_name and alternative names
        resolved_by_alternatives = ['resolved_by_user_name', 'resolved_by_name', 'resolved_by']
        resolved_by_found = None
        for alt in resolved_by_alternatives:
            if alt in df.columns:
                resolved_by_found = alt
                logging.info(f"‚úì Found resolved_by column: '{alt}'")
                break
        
        if not resolved_by_found:
            logging.warning(f"‚ö†Ô∏è  No resolved_by column found in CSV (checked: {resolved_by_alternatives})")
            # Check case-insensitive
            for csv_col in csv_columns:
                if 'resolved' in csv_col.lower() and 'by' in csv_col.lower():
                    logging.info(f"   Found potential resolved_by column: '{csv_col}'")
        
        # Create column mapping (case-insensitive match)
        column_mapping = {}
        for csv_col in csv_columns:
            csv_col_lower = csv_col.lower()
            if csv_col_lower in table_columns_lower:
                table_col = table_columns_lower[csv_col_lower]
                if csv_col != table_col:
                    column_mapping[csv_col] = table_col
            # Map alternative resolved_by column names
            elif csv_col_lower == 'resolved_by_name' and 'resolved_by_user_name' in table_columns:
                column_mapping[csv_col] = 'resolved_by_user_name'
                logging.info(f"   Mapping '{csv_col}' -> 'resolved_by_user_name'")
        
        if column_mapping:
            df.rename(columns=column_mapping, inplace=True)
            logging.info(f"‚úì Renamed {len(column_mapping)} columns to match table case")
        
        # Find missing columns in CSV
        metadata_cols = {'created_at', 'updated_at'}
        missing_in_csv = []
        for table_col in table_columns:
            if table_col.lower() not in [c.lower() for c in df.columns] and table_col not in metadata_cols:
                missing_in_csv.append(table_col)
                df[table_col] = None
        
        if missing_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV is missing {len(missing_in_csv)} table columns (set to NULL):")
            for col in missing_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(missing_in_csv) > 10:
                logging.warning(f"   ... and {len(missing_in_csv) - 10} more")
        
        # Fill empty resolved_by_user_name from fallback fields (similar to daily_load_incidents.py)
        if 'resolved_by_user_name' in df.columns:
            initial_empty = ((df['resolved_by_user_name'].isna()) | (df['resolved_by_user_name'] == '')).sum()
            total_count = len(df)
            
            if initial_empty > 0:
                logging.info(f"\n‚è≥ Found {initial_empty:,}/{total_count:,} empty resolved_by_user_name values, attempting fallback...")
                
                # Try to fill from resolved_by_gpn if available
                if 'resolved_by_gpn' in df.columns:
                    empty_mask = (df['resolved_by_user_name'].isna()) | (df['resolved_by_user_name'] == '')
                    gpn_mask = empty_mask & df['resolved_by_gpn'].notna() & (df['resolved_by_gpn'].astype(str) != '')
                    if gpn_mask.any():
                        df.loc[gpn_mask, 'resolved_by_user_name'] = df.loc[gpn_mask, 'resolved_by_gpn'].astype(str)
                        logging.info(f"   ‚úì Filled {gpn_mask.sum():,} values from resolved_by_gpn")
                
                # Try to fill from sys_updated_by if available
                if 'sys_updated_by' in df.columns:
                    empty_mask = (df['resolved_by_user_name'].isna()) | (df['resolved_by_user_name'] == '')
                    updated_by_mask = empty_mask & df['sys_updated_by'].notna() & (df['sys_updated_by'].astype(str) != '')
                    if updated_by_mask.any():
                        df.loc[updated_by_mask, 'resolved_by_user_name'] = df.loc[updated_by_mask, 'sys_updated_by'].astype(str)
                        logging.info(f"   ‚úì Filled {updated_by_mask.sum():,} values from sys_updated_by")
                
                final_empty = ((df['resolved_by_user_name'].isna()) | (df['resolved_by_user_name'] == '')).sum()
                filled_count = initial_empty - final_empty
                if filled_count > 0:
                    logging.info(f"   ‚úÖ Successfully filled {filled_count:,} empty resolved_by_user_name values")
                if final_empty > 0:
                    logging.warning(f"   ‚ö†Ô∏è  {final_empty:,} resolved_by_user_name values remain empty")
            else:
                logging.info(f"   ‚úì All {total_count:,} resolved_by_user_name values are populated")
        
        # Find extra columns in CSV (not in table)
        extra_in_csv = []
        for csv_col in df.columns:
            if csv_col not in table_columns and csv_col not in metadata_cols:
                extra_in_csv.append(csv_col)
        
        if extra_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV has {len(extra_in_csv)} extra columns (will be ignored):")
            for col in extra_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(extra_in_csv) > 10:
                logging.warning(f"   ... and {len(extra_in_csv) - 10} more")
        
        # Filter DataFrame to only include columns that exist in table
        columns_to_insert = [col for col in table_columns if col not in metadata_cols]
        df_final = df[[col for col in columns_to_insert if col in df.columns]].copy()
        
        # Validate and truncate VARCHAR columns to match table constraints
        logging.info(f"\n‚è≥ Validating data against table constraints...")
        try:
            with engine.connect() as conn:
                # Get column information including max lengths
                result = conn.execute(text(f"""
                    SELECT 
                        column_name,
                        data_type,
                        character_maximum_length
                    FROM information_schema.columns
                    WHERE table_name = '{table_name}'
                    AND table_schema = 'public'
                    AND character_maximum_length IS NOT NULL
                """))
                varchar_columns = {row[0]: row[2] for row in result.fetchall()}
            
            logging.info(f"   Found {len(varchar_columns)} VARCHAR columns with length constraints")
            
            # Truncate values that exceed VARCHAR length
            truncated_count = 0
            truncation_details = {}
            
            for col_name, max_length in varchar_columns.items():
                if col_name in df_final.columns:
                    try:
                        # Check for values exceeding length
                        mask = df_final[col_name].notna()
                        if mask.any():
                            str_series = df_final.loc[mask, col_name].astype(str)
                            exceeds_mask = str_series.str.len() > max_length
                            if exceeds_mask.any():
                                count = exceeds_mask.sum()
                                truncated_count += count
                                max_exceeded = str_series.loc[exceeds_mask].str.len().max()
                                truncation_details[col_name] = {
                                    'count': count,
                                    'max_length': max_length,
                                    'max_exceeded': max_exceeded
                                }
                                df_final.loc[mask & exceeds_mask, col_name] = str_series.loc[exceeds_mask].str[:max_length]
                                logging.warning(f"   ‚ö†Ô∏è  Truncated {count:,} values in '{col_name}' (exceeded VARCHAR({max_length}), max was {max_exceeded})")
                    except Exception as e:
                        logging.error(f"   ‚ùå Error validating '{col_name}': {e}")
                        logging.error(traceback.format_exc())
            
            if truncated_count > 0:
                logging.warning(f"   ‚ö†Ô∏è  Total truncated values: {truncated_count:,} across {len(truncation_details)} columns")
                for col, details in truncation_details.items():
                    logging.warning(f"      - {col}: {details['count']:,} values (max length: {details['max_exceeded']})")
            else:
                logging.info(f"   ‚úì All VARCHAR values within constraints")
        except Exception as e:
            logging.error(f"   ‚ùå Error validating VARCHAR constraints: {e}")
            logging.error(traceback.format_exc())
            logging.warning(f"   ‚ö†Ô∏è  Continuing without constraint validation...")
        
        # Check existing row count
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
            existing_rows = result.fetchone()[0]
        logging.info(f"‚úì Table currently has {existing_rows:,} rows")
        
        # Truncate table if requested
        if truncate:
            logging.info(f"\n‚è≥ Truncating table '{table_name}'...")
            with engine.connect() as conn:
                conn.execute(text(f'TRUNCATE TABLE "{table_name}";'))
                conn.commit()
            logging.info(f"‚úì Table truncated")
        
        # Push to database with progress bar
        total_rows = len(df_final)
        chunksize = 1000
        num_chunks = (total_rows + chunksize - 1) // chunksize
        
        logging.info(f"\n‚è≥ Writing {total_rows:,} rows to PostgreSQL table '{table_name}'...")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info(f"   Columns: {len(df_final.columns)} columns matching table structure")
        logging.info(f"   Chunk size: {chunksize:,} rows per batch")
        logging.info(f"   Total batches: {num_chunks:,}")
        
        # Insert with progress bar and error handling
        if TQDM_AVAILABLE:
            with tqdm(total=total_rows, desc="Inserting rows", unit="row", unit_scale=True, ncols=100) as pbar:
                for i in range(0, total_rows, chunksize):
                    chunk_df = df_final.iloc[i:i+chunksize].copy()
                    try:
                        chunk_df.to_sql(
                            name=table_name,
                            con=engine,
                            schema='public',
                            if_exists='append',  # Append since we truncated if needed
                            index=False,
                            method='multi',  # Use multi-row INSERT for better performance
                            chunksize=chunksize
                        )
                        pbar.update(len(chunk_df))
                        # Update description with current progress
                        pbar.set_postfix({
                            'batch': f"{i//chunksize + 1}/{num_chunks}",
                            'rows': f"{min(i+chunksize, total_rows):,}/{total_rows:,}"
                        })
                    except Exception as e:
                        batch_num = i//chunksize + 1
                        logging.error(f"   ‚ùå Error inserting batch {batch_num}/{num_chunks}: {e}")
                        logging.error(f"      Exception type: {type(e).__name__}")
                        logging.error(f"      Batch range: rows {i} to {min(i+chunksize, total_rows)}")
                        logging.error(f"      Traceback: {traceback.format_exc()}")
                        
                        # Try inserting row by row to identify problematic rows
                        logging.warning(f"   ‚ö†Ô∏è  Attempting row-by-row insertion for this batch...")
                        failed_rows = 0
                        successful_rows = 0
                        
                        for idx, row in chunk_df.iterrows():
                            try:
                                row.to_frame().T.to_sql(
                                    name=table_name,
                                    con=engine,
                                    schema='public',
                                    if_exists='append',
                                    index=False
                                )
                                pbar.update(1)
                                successful_rows += 1
                            except Exception as row_error:
                                failed_rows += 1
                                incident_num = row.get('incident_number', 'N/A')
                                logging.error(f"   ‚ùå Failed to insert row {idx} (incident: {incident_num}): {row_error}")
                                logging.error(f"      Exception type: {type(row_error).__name__}")
                                
                                # Log problematic column values if available
                                if hasattr(row_error, 'args') and row_error.args:
                                    logging.error(f"      Error details: {row_error.args[0] if row_error.args else 'N/A'}")
                                
                                # Only log first 5 failed rows in detail to avoid spam
                                if failed_rows <= 5:
                                    problem_cols = []
                                    for col in df_final.columns:
                                        val = row.get(col)
                                        if pd.notna(val) and isinstance(val, str) and len(str(val)) > 100:
                                            problem_cols.append(f"{col} (length: {len(str(val))})")
                                    if problem_cols:
                                        logging.error(f"      Potentially problematic columns: {', '.join(problem_cols[:3])}")
                        
                        if failed_rows > 0:
                            logging.warning(f"   ‚ö†Ô∏è  Batch {batch_num}: {failed_rows} rows failed, {successful_rows} succeeded")
                        else:
                            logging.info(f"   ‚úì All rows in batch {batch_num} inserted successfully")
        else:
            # Fallback: insert without progress bar but with logging
            logging.info("   (Install tqdm for progress bar: pip install tqdm)")
            for i in range(0, total_rows, chunksize):
                chunk_df = df_final.iloc[i:i+chunksize].copy()
                try:
                    chunk_df.to_sql(
                        name=table_name,
                        con=engine,
                        schema='public',
                        if_exists='append',  # Append since we truncated if needed
                        index=False,
                        method='multi',  # Use multi-row INSERT for better performance
                        chunksize=chunksize
                    )
                    if (i // chunksize + 1) % 10 == 0 or i + chunksize >= total_rows:
                        logging.info(f"   Inserted {min(i+chunksize, total_rows):,}/{total_rows:,} rows...")
                except Exception as e:
                    batch_num = i//chunksize + 1
                    logging.error(f"   ‚ùå Error inserting batch {batch_num}/{num_chunks}: {e}")
                    logging.error(f"      Exception type: {type(e).__name__}")
                    logging.error(f"      Batch range: rows {i} to {min(i+chunksize, total_rows)}")
                    logging.error(f"      Traceback: {traceback.format_exc()}")
                    raise
        
        logging.info(f"‚úÖ Insertion completed!")
        
        engine.dispose()
        
        # Verify final row count
        logging.info(f"\n‚è≥ Verifying final row count...")
        try:
            engine = create_engine(connection_string, pool_pre_ping=True)
            with engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
                final_rows = result.fetchone()[0]
            engine.dispose()
            logging.info(f"‚úì Verified: {final_rows:,} rows in table")
        except Exception as e:
            logging.error(f"   ‚ùå Error verifying row count: {e}")
            logging.error(traceback.format_exc())
            final_rows = "Unknown"
        
        logging.info(f"\n{'='*80}")
        logging.info("‚úÖ SUCCESS!")
        logging.info(f"{'='*80}")
        logging.info(f"‚úì Wrote {len(df_final):,} rows to PostgreSQL")
        logging.info(f"‚úì Database: {database}")
        logging.info(f"‚úì Table: {table_name}")
        logging.info(f"‚úì Total rows in table now: {final_rows:,}")
        if not truncate:
            logging.info(f"   (Previous: {existing_rows:,}, Added: {len(df_final):,})")
        logging.info(f"{'='*80}\n")
        
        return True
        
    except Exception as e:
        logging.error(f"\n{'='*80}")
        logging.error("‚ùå FAILED TO EXPORT TO POSTGRESQL")
        logging.error(f"{'='*80}")
        logging.error(f"Error: {e}")
        logging.error(f"Error type: {type(e).__name__}")
        logging.error(f"\nFull traceback:")
        logging.error(traceback.format_exc())
        logging.error(f"{'='*80}\n")
        return False

def main():
    """Main execution function."""
    setup_logging()
    
    parser = argparse.ArgumentParser(
        description='Push categorized incident CSV to PostgreSQL',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv --truncate

Environment variables (optional):
  PGHOST=localhost
  PGPORT=5432
  PGDATABASE=service_automation_db
  PGUSER=postgres
  PGPASSWORD=your_password

Or use .pgpass file for password-less authentication
        """
    )
    
    parser.add_argument('csv_file', help='Path to categorized incident CSV file')
    parser.add_argument('--truncate', action='store_true', 
                       help='Truncate table before inserting (overwrite mode)')
    parser.add_argument('--table', type=str, default=None,
                       help='Table name (default: incident_data, will be lowercase)')
    parser.add_argument('--database', type=str, default=None,
                       help='Database name (default: from config)')
    parser.add_argument('--no-create-table', action='store_true',
                       help='Do not create table if it does not exist (default: auto-create)')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.csv_file):
        logging.error(f"‚úó CSV file not found: {args.csv_file}")
        sys.exit(1)
    
    # Push to PostgreSQL
    success = push_incidents_to_postgresql(
        args.csv_file,
        table_name=args.table,
        database=args.database,
        truncate=args.truncate,
        create_table=not args.no_create_table
    )
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()

