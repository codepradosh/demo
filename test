#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Push categorized incident CSV file to PostgreSQL table Incident_Data
Usage: python push_incidents_to_postgres.py <csv_file_path> [--truncate]
"""

import os
import sys
import pandas as pd
import traceback
import logging
from datetime import datetime
import argparse
from urllib.parse import quote_plus

# PostgreSQL client
try:
    import psycopg2
    from sqlalchemy import create_engine, text, inspect
    from sqlalchemy.types import Integer, BigInteger, Float, Boolean, DateTime, Text, String, Date
    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("ERROR: PostgreSQL libraries not available.")
    print("Please install: pip install psycopg2-binary sqlalchemy pandas")
    sys.exit(1)

# ------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------
def setup_logging():
    """Set up logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

# ------------------------------------------------------------------------
# PostgreSQL Configuration
# ------------------------------------------------------------------------
POSTGRES_CONFIG = {
    'host': os.getenv('PGHOST', 'xd1e1159676dev.ubscloud-prod.msad.ubs.net'),
    'port': os.getenv('PGPORT', '5432'),
    'database': os.getenv('PGDATABASE', 'service_automation_db'),
    'user': os.getenv('PGUSER', 'powerbi_user'),
    'password': os.getenv('PGPASSWORD', 'Report@123'),
    'table_name': 'incident_data'  # Changed to lowercase incident_data
}

# ------------------------------------------------------------------------
# Data Type Inference
# ------------------------------------------------------------------------
def infer_postgres_type(series, col_name):
    """
    Infer PostgreSQL data type from pandas Series.
    
    Parameters
    ----------
    series : pd.Series
        The pandas Series to analyze
    col_name : str
        Column name (for special handling)
    
    Returns
    -------
    sqlalchemy.types.TypeEngine
        SQLAlchemy type for PostgreSQL
    """
    # Remove null values for type inference
    non_null = series.dropna()
    
    if len(non_null) == 0:
        # All nulls - default to Text
        return Text
    
    # Check for boolean
    if series.dtype == 'bool' or non_null.dtype == 'bool':
        return Boolean
    
    # Check for datetime
    if pd.api.types.is_datetime64_any_dtype(series):
        return DateTime
    
    # Check for date
    if pd.api.types.is_datetime64_any_dtype(series) and 'date' in col_name.lower():
        # Try to parse as date if it's a date column
        try:
            pd.to_datetime(non_null.iloc[0])
            return Date
        except:
            pass
    
    # Check for integer
    if pd.api.types.is_integer_dtype(series):
        # Check if values fit in regular integer or need bigint
        if non_null.abs().max() < 2147483647:  # PostgreSQL INTEGER max
            return Integer
        else:
            return BigInteger
    
    # Check for float
    if pd.api.types.is_float_dtype(series):
        return Float
    
    # Check if numeric string can be converted to int
    if series.dtype == 'object':
        # Try to convert to numeric
        try:
            numeric_series = pd.to_numeric(non_null, errors='coerce')
            if numeric_series.notna().sum() == len(non_null):
                # All values are numeric
                if numeric_series.abs().max() < 2147483647:
                    return Integer
                else:
                    return BigInteger
        except:
            pass
    
    # Check string length for VARCHAR vs TEXT
    if series.dtype == 'object':
        max_length = non_null.astype(str).str.len().max()
        if max_length > 0 and max_length < 255:
            # Use VARCHAR with some buffer
            return String(max_length + 50)  # Add 50 char buffer
        else:
            return Text
    
    # Default to Text
    return Text

def create_table_from_dataframe(engine, df, table_name, schema='public'):
    """
    Create PostgreSQL table from DataFrame with inferred data types.
    
    Parameters
    ----------
    engine : sqlalchemy.engine.Engine
        SQLAlchemy engine
    df : pd.DataFrame
        DataFrame to create table from
    table_name : str
        Table name (will be created in lowercase)
    schema : str
        Schema name (default: 'public')
    
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    try:
        logging.info(f"\n{'='*80}")
        logging.info(f"üìã CREATING TABLE: {table_name}")
        logging.info(f"{'='*80}")
        
        # Infer column types
        column_types = {}
        for col in df.columns:
            pg_type = infer_postgres_type(df[col], col)
            column_types[col] = pg_type
            logging.info(f"   {col}: {pg_type}")
        
        # Build CREATE TABLE SQL
        columns_sql = []
        for col in df.columns:
            pg_type = column_types[col]
            
            # Convert SQLAlchemy type to PostgreSQL type string
            if pg_type == Boolean:
                type_str = "BOOLEAN"
            elif pg_type == Integer:
                type_str = "INTEGER"
            elif pg_type == BigInteger:
                type_str = "BIGINT"
            elif pg_type == Float:
                type_str = "DOUBLE PRECISION"
            elif pg_type == DateTime:
                type_str = "TIMESTAMP"
            elif pg_type == Date:
                type_str = "DATE"
            elif isinstance(pg_type, String):
                type_str = f"VARCHAR({pg_type.length})"
            else:
                type_str = "TEXT"
            
            # Escape column name if needed
            col_escaped = f'"{col}"'
            columns_sql.append(f"{col_escaped} {type_str}")
        
        # Add created_at and updated_at timestamps
        columns_sql.append('"created_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        columns_sql.append('"updated_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP')
        
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {schema}."{table_name}" (
            {', '.join(columns_sql)}
        );
        """
        
        logging.info(f"\n‚è≥ Creating table '{table_name}'...")
        with engine.connect() as conn:
            conn.execute(text(create_table_sql))
            conn.commit()
        
        logging.info(f"‚úÖ Table '{table_name}' created successfully")
        logging.info(f"   Columns: {len(df.columns) + 2} (including created_at, updated_at)")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error creating table: {e}")
        logging.error(traceback.format_exc())
        return False

# ------------------------------------------------------------------------
# Main Function
# ------------------------------------------------------------------------
def push_incidents_to_postgresql(csv_file_path, table_name=None, database=None, 
                                 user=None, host=None, port=None, password=None,
                                 truncate=False, create_table=True):
    """
    Push categorized incident CSV file to PostgreSQL table.
    
    Parameters
    ----------
    csv_file_path : str
        Path to the categorized CSV file to import
    table_name : str, optional
        Target table name (default: 'Incident_Data')
    database : str, optional
        Database name (default: from config or 'service_automation_db')
    user : str, optional
        PostgreSQL username (default: from config)
    host : str, optional
        PostgreSQL host (default: from config)
    port : str, optional
        PostgreSQL port (default: from config or '5432')
    password : str, optional
        PostgreSQL password (default: from environment variable PGPASSWORD)
    truncate : bool, optional
        If True, truncate table before inserting (default: False)
        
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    if not POSTGRES_AVAILABLE:
        logging.error("‚úó PostgreSQL libraries not available. Cannot export to PostgreSQL.")
        return False
    
    if not os.path.exists(csv_file_path):
        logging.error(f"‚úó CSV file not found: {csv_file_path}")
        return False
    
    # Use provided values or fall back to config/environment
    table_name = (table_name or POSTGRES_CONFIG['table_name']).lower()  # Ensure lowercase
    database = database or POSTGRES_CONFIG['database']
    user = user or POSTGRES_CONFIG['user']
    host = host or POSTGRES_CONFIG['host']
    port = port or POSTGRES_CONFIG['port']
    password = password or POSTGRES_CONFIG['password']
    
    try:
        logging.info("="*80)
        logging.info("üì§ Pushing Incident CSV to PostgreSQL")
        logging.info("="*80)
        logging.info(f"   CSV file: {csv_file_path}")
        logging.info(f"   Database: {database}")
        logging.info(f"   Table: {table_name}")
        logging.info(f"   Host: {host}:{port}")
        logging.info(f"   User: {user}")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info(f"   Create table: {create_table}")
        logging.info("="*80)
        
        # Read CSV file
        logging.info(f"\n‚è≥ Reading CSV file...")
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8', low_memory=False)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_file_path, encoding='latin-1', low_memory=False)
            except:
                df = pd.read_csv(csv_file_path, encoding='cp1252', low_memory=False)
        
        logging.info(f"‚úì Read {len(df):,} rows from CSV")
        logging.info(f"‚úì Columns: {len(df.columns)} ({', '.join(df.columns[:5])}...)")
        
        # Convert date columns to datetime if they're strings
        date_columns = ['opened_at', 'closed_at', 'resolved_at', 'sys_created_on', 'sys_updated_on', 
                        'created_on', 'updated_on', 'opened_date', 'closed_date']
        for col in df.columns:
            if any(date_keyword in col.lower() for date_keyword in ['date', 'time', 'created', 'updated', 'opened', 'closed', 'resolved']):
                if df[col].dtype == 'object':
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                        logging.info(f"‚úì Converted {col} to datetime")
                    except:
                        pass
        
        # Convert boolean columns
        bool_columns = ['active', 'made_sla']
        for col in bool_columns:
            if col in df.columns:
                df[col] = df[col].astype(bool)
        
        # Convert integer columns
        int_columns = ['reassignment_count', 'reopen_count']
        for col in int_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
        
        # Create connection string with URL-encoded password
        # Fix: Use quote_plus to properly encode password (handles @, #, etc.)
        if password:
            # URL-encode the password to handle special characters like @
            encoded_password = quote_plus(password)
            connection_string = f"postgresql://{user}:{encoded_password}@{host}:{port}/{database}"
        else:
            # Try without password (trust authentication or .pgpass)
            connection_string = f"postgresql://{user}@{host}:{port}/{database}"
        
        # Create SQLAlchemy engine
        logging.info(f"\n‚è≥ Connecting to PostgreSQL...")
        engine = create_engine(connection_string, pool_pre_ping=True)
        
        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logging.info(f"‚úì Connected to PostgreSQL successfully")
        
        # Check if table exists
        logging.info(f"\n‚è≥ Checking if table '{table_name}' exists...")
        inspector = inspect(engine)
        table_exists = inspector.has_table(table_name, schema='public')
        
        if not table_exists:
            if create_table:
                logging.info(f"‚ö†Ô∏è  Table '{table_name}' does not exist. Creating it...")
                success = create_table_from_dataframe(engine, df, table_name)
                if not success:
                    logging.error(f"‚úó Failed to create table '{table_name}'")
                    engine.dispose()
                    return False
            else:
                logging.error(f"‚úó Table '{table_name}' does not exist in database '{database}'")
                logging.error(f"  Use --create-table to create it automatically")
                engine.dispose()
                return False
        else:
            logging.info(f"‚úì Table '{table_name}' exists")
        
        # Get table column names
        logging.info(f"\n‚è≥ Fetching table column names...")
        table_columns = [col['name'] for col in inspector.get_columns(table_name, schema='public')]
        
        col_count = len(table_columns)
        logging.info(f"‚úì Table has {col_count} columns")
        
        # Validate CSV columns match table columns
        logging.info(f"\n‚è≥ Validating column names...")
        
        csv_columns = [col for col in df.columns]
        table_columns_lower = {col.lower(): col for col in table_columns}
        
        # Create column mapping (case-insensitive match)
        column_mapping = {}
        for csv_col in csv_columns:
            csv_col_lower = csv_col.lower()
            if csv_col_lower in table_columns_lower:
                table_col = table_columns_lower[csv_col_lower]
                if csv_col != table_col:
                    column_mapping[csv_col] = table_col
        
        if column_mapping:
            df.rename(columns=column_mapping, inplace=True)
            logging.info(f"‚úì Renamed {len(column_mapping)} columns to match table case")
        
        # Find missing columns in CSV
        metadata_cols = {'created_at', 'updated_at'}
        missing_in_csv = []
        for table_col in table_columns:
            if table_col.lower() not in [c.lower() for c in df.columns] and table_col not in metadata_cols:
                missing_in_csv.append(table_col)
                df[table_col] = None
        
        if missing_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV is missing {len(missing_in_csv)} table columns (set to NULL):")
            for col in missing_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(missing_in_csv) > 10:
                logging.warning(f"   ... and {len(missing_in_csv) - 10} more")
        
        # Find extra columns in CSV (not in table)
        extra_in_csv = []
        for csv_col in df.columns:
            if csv_col not in table_columns and csv_col not in metadata_cols:
                extra_in_csv.append(csv_col)
        
        if extra_in_csv:
            logging.warning(f"\n‚ö†Ô∏è  CSV has {len(extra_in_csv)} extra columns (will be ignored):")
            for col in extra_in_csv[:10]:
                logging.warning(f"   - {col}")
            if len(extra_in_csv) > 10:
                logging.warning(f"   ... and {len(extra_in_csv) - 10} more")
        
        # Filter DataFrame to only include columns that exist in table
        columns_to_insert = [col for col in table_columns if col not in metadata_cols]
        df_final = df[[col for col in columns_to_insert if col in df.columns]].copy()
        
        # Check existing row count
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
            existing_rows = result.fetchone()[0]
        logging.info(f"‚úì Table currently has {existing_rows:,} rows")
        
        # Truncate table if requested
        if truncate:
            logging.info(f"\n‚è≥ Truncating table '{table_name}'...")
            with engine.connect() as conn:
                conn.execute(text(f'TRUNCATE TABLE "{table_name}";'))
                conn.commit()
            logging.info(f"‚úì Table truncated")
        
        # Push to database
        logging.info(f"\n‚è≥ Writing {len(df_final):,} rows to PostgreSQL table '{table_name}'...")
        logging.info(f"   Mode: {'TRUNCATE + INSERT' if truncate else 'APPEND'}")
        logging.info(f"   Columns: {len(df_final.columns)} columns matching table structure")
        logging.info(f"   Chunk size: 1000 rows per batch")
        
        df_final.to_sql(
            name=table_name,
            con=engine,
            schema='public',
            if_exists='append',  # Append since we truncated if needed
            index=False,
            method='multi',  # Use multi-row INSERT for better performance
            chunksize=1000
        )
        
        engine.dispose()
        
        # Verify final row count
        engine = create_engine(connection_string, pool_pre_ping=True)
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM \"{table_name}\";"))
            final_rows = result.fetchone()[0]
        engine.dispose()
        
        logging.info(f"\n{'='*80}")
        logging.info("‚úÖ SUCCESS!")
        logging.info(f"{'='*80}")
        logging.info(f"‚úì Wrote {len(df_final):,} rows to PostgreSQL")
        logging.info(f"‚úì Database: {database}")
        logging.info(f"‚úì Table: {table_name}")
        logging.info(f"‚úì Total rows in table now: {final_rows:,}")
        if not truncate:
            logging.info(f"   (Previous: {existing_rows:,}, Added: {len(df_final):,})")
        logging.info(f"{'='*80}\n")
        
        return True
        
    except Exception as e:
        logging.error(f"\n‚úó Failed to export to PostgreSQL: {e}")
        logging.error(traceback.format_exc())
        return False

def main():
    """Main execution function."""
    setup_logging()
    
    parser = argparse.ArgumentParser(
        description='Push categorized incident CSV to PostgreSQL',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv
  python push_incidents_to_postgres.py incidents_categorized_20250101_120000.csv --truncate

Environment variables (optional):
  PGHOST=localhost
  PGPORT=5432
  PGDATABASE=service_automation_db
  PGUSER=postgres
  PGPASSWORD=your_password

Or use .pgpass file for password-less authentication
        """
    )
    
    parser.add_argument('csv_file', help='Path to categorized incident CSV file')
    parser.add_argument('--truncate', action='store_true', 
                       help='Truncate table before inserting (overwrite mode)')
    parser.add_argument('--table', type=str, default=None,
                       help='Table name (default: incident_data, will be lowercase)')
    parser.add_argument('--database', type=str, default=None,
                       help='Database name (default: from config)')
    parser.add_argument('--no-create-table', action='store_true',
                       help='Do not create table if it does not exist (default: auto-create)')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.csv_file):
        logging.error(f"‚úó CSV file not found: {args.csv_file}")
        sys.exit(1)
    
    # Push to PostgreSQL
    success = push_incidents_to_postgresql(
        args.csv_file,
        table_name=args.table,
        database=args.database,
        truncate=args.truncate,
        create_table=not args.no_create_table
    )
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()

