#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV Column Data Type Analyzer (Improved)
Analyzes a CSV file and displays data types, sample values, and statistics for each column.
Usage: python analyze_csv_columns.py <csv_file_path>
"""

import os
import sys
import pandas as pd
import numpy as np
import re

def is_date_column(col_name):
    """Check if column name suggests it's a date/time column"""
    date_keywords = ['date', 'time', 'created', 'updated', 'opened', 'closed', 'resolved', 'at']
    col_lower = col_name.lower()
    # Exclude user/GPN columns from date detection
    exclude_keywords = ['user_name', 'gpn', 'by_name', 'by_gpn', 'updated_by', 'created_by']
    if any(excl in col_lower for excl in exclude_keywords):
        return False
    return any(keyword in col_lower for keyword in date_keywords)

def is_gpn_column(col_name):
    """Check if column is a GPN (Global Personnel Number) column"""
    return 'gpn' in col_name.lower()

def is_user_name_column(col_name):
    """Check if column is a user name column"""
    return 'user_name' in col_name.lower() or 'by_name' in col_name.lower()

def analyze_csv_columns(csv_file_path):
    """
    Analyze CSV file and display column information including data types.
    """
    if not os.path.exists(csv_file_path):
        print(f"âŒ CSV file not found: {csv_file_path}")
        return
    
    print("=" * 100)
    print("ðŸ“Š CSV COLUMN DATA TYPE ANALYZER (IMPROVED)")
    print("=" * 100)
    print(f"ðŸ“ File: {csv_file_path}\n")
    
    # Read CSV file
    print("â³ Reading CSV file...")
    try:
        df = pd.read_csv(csv_file_path, encoding='utf-8', low_memory=False)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(csv_file_path, encoding='latin-1', low_memory=False)
            print("   âš ï¸  Used latin-1 encoding")
        except:
            df = pd.read_csv(csv_file_path, encoding='cp1252', low_memory=False)
            print("   âš ï¸  Used cp1252 encoding")
    
    print(f"âœ… Read {len(df):,} rows, {len(df.columns)} columns\n")
    
    # Analyze each column
    print("=" * 100)
    print("COLUMN ANALYSIS")
    print("=" * 100)
    
    results = []
    
    for idx, col in enumerate(df.columns, 1):
        print(f"\n[{idx}/{len(df.columns)}] Column: '{col}'")
        print("-" * 100)
        
        # Basic info
        total_count = len(df[col])
        non_null_count = df[col].notna().sum()
        null_count = total_count - non_null_count
        null_percentage = (null_count / total_count * 100) if total_count > 0 else 0
        
        print(f"   Total rows: {total_count:,}")
        print(f"   Non-null: {non_null_count:,} ({100-null_percentage:.1f}%)")
        print(f"   Null/Empty: {null_count:,} ({null_percentage:.1f}%)")
        
        # Pandas dtype
        pandas_dtype = df[col].dtype
        print(f"   Pandas dtype: {pandas_dtype}")
        
        # Actual data type analysis
        non_null_series = df[col].dropna()
        
        if len(non_null_series) == 0:
            print(f"   âš ï¸  All values are NULL/Empty")
            inferred_type = "UNKNOWN (all null)"
            sample_values = ["N/A"]
            max_length = None
        else:
            # Special handling for GPN columns - should be VARCHAR, not numeric
            if is_gpn_column(col):
                inferred_type = "VARCHAR (GPN)"
                str_series = non_null_series.astype(str)
                max_length = str_series.str.len().max()
                sample_values = str_series.head(3).tolist()
                print(f"   âœ… Detected as: VARCHAR (GPN column)")
                print(f"   String length: max={max_length}")
            
            # Special handling for user name columns - should be VARCHAR, not date
            elif is_user_name_column(col):
                inferred_type = "VARCHAR (User Name)"
                str_series = non_null_series.astype(str)
                max_length = str_series.str.len().max()
                sample_values = str_series.head(3).tolist()
                print(f"   âœ… Detected as: VARCHAR (User Name column)")
                print(f"   String length: max={max_length}")
            
            # Check for datetime (only for date-like column names)
            elif is_date_column(col) and pd.api.types.is_datetime64_any_dtype(df[col]):
                inferred_type = "DATETIME"
                sample_values = non_null_series.head(3).astype(str).tolist()
                max_length = None
                print(f"   âœ… Detected as: DATETIME")
            
            # Check for datetime string (only for date-like column names)
            elif is_date_column(col) and pandas_dtype == 'object':
                # Try to parse as date
                str_series = non_null_series.astype(str)
                try:
                    test_dates = pd.to_datetime(str_series.head(100), errors='coerce')
                    if test_dates.notna().sum() > len(test_dates) * 0.8:  # 80% parseable
                        inferred_type = "DATETIME (string)"
                        max_length = str_series.str.len().max()
                        sample_values = str_series.head(3).tolist()
                        print(f"   âœ… Detected as: DATETIME (stored as string)")
                    else:
                        # Not a date, treat as string
                        inferred_type = "STRING/VARCHAR"
                        max_length = str_series.str.len().max()
                        sample_values = str_series.head(3).tolist()
                        print(f"   âœ… Detected as: STRING/VARCHAR")
                except:
                    inferred_type = "STRING/VARCHAR"
                    max_length = str_series.str.len().max()
                    sample_values = str_series.head(3).tolist()
                    print(f"   âœ… Detected as: STRING/VARCHAR")
            
            # Check for boolean
            elif pandas_dtype == 'bool' or (pandas_dtype == 'object' and 
                    non_null_series.astype(str).str.lower().isin(['true', 'false', '1', '0', 'yes', 'no', 't', 'f']).any()):
                # Check if it's actually boolean
                unique_lower = non_null_series.astype(str).str.lower().unique()
                bool_like = all(v in ['true', 'false', '1', '0', 'yes', 'no', 't', 'f', 'nan', 'none', ''] for v in unique_lower)
                if bool_like and len(unique_lower) <= 4:
                    inferred_type = "BOOLEAN"
                    unique_vals = non_null_series.unique()[:5]
                    sample_values = [str(v) for v in unique_vals]
                    max_length = None
                    print(f"   âœ… Detected as: BOOLEAN")
                else:
                    inferred_type = "STRING/VARCHAR"
                    str_series = non_null_series.astype(str)
                    max_length = str_series.str.len().max()
                    sample_values = str_series.head(3).tolist()
                    print(f"   âœ… Detected as: STRING/VARCHAR")
            
            # Check for integer
            elif pd.api.types.is_integer_dtype(df[col]):
                inferred_type = "INTEGER"
                min_val = non_null_series.min()
                max_val = non_null_series.max()
                sample_values = non_null_series.head(3).tolist()
                max_length = None
                print(f"   âœ… Detected as: INTEGER")
                print(f"   Range: {min_val} to {max_val}")
            
            # Check for float
            elif pd.api.types.is_float_dtype(df[col]):
                # Check if it's actually numeric or just NaN representation
                if non_null_series.isna().all():
                    inferred_type = "UNKNOWN (all null)"
                    max_length = None
                    sample_values = ["N/A"]
                else:
                    inferred_type = "FLOAT"
                    min_val = non_null_series.min()
                    max_val = non_null_series.max()
                    sample_values = non_null_series.head(3).tolist()
                    max_length = None
                    print(f"   âœ… Detected as: FLOAT")
                    print(f"   Range: {min_val:.2f} to {max_val:.2f}")
            
            # Check if numeric string
            elif pandas_dtype == 'object':
                # Try to convert to numeric
                numeric_series = pd.to_numeric(non_null_series, errors='coerce')
                numeric_count = numeric_series.notna().sum()
                
                if numeric_count == len(non_null_series) and numeric_count > 0:
                    # All numeric
                    if numeric_series.dtype in ['int64', 'Int64']:
                        inferred_type = "INTEGER (string)"
                        min_val = numeric_series.min()
                        max_val = numeric_series.max()
                        print(f"   âœ… Detected as: INTEGER (stored as string)")
                        print(f"   Range: {min_val} to {max_val}")
                        max_length = None
                    else:
                        inferred_type = "FLOAT (string)"
                        min_val = numeric_series.min()
                        max_val = numeric_series.max()
                        print(f"   âœ… Detected as: FLOAT (stored as string)")
                        print(f"   Range: {min_val:.2f} to {max_val:.2f}")
                        max_length = None
                    sample_values = non_null_series.head(3).tolist()
                else:
                    # String type
                    str_series = non_null_series.astype(str)
                    max_length = str_series.str.len().max()
                    min_length = str_series.str.len().min()
                    avg_length = str_series.str.len().mean()
                    
                    inferred_type = "STRING/VARCHAR"
                    print(f"   âœ… Detected as: STRING/VARCHAR")
                    print(f"   String length: min={min_length}, max={max_length}, avg={avg_length:.1f}")
                    
                    # Show unique value count
                    unique_count = non_null_series.nunique()
                    print(f"   Unique values: {unique_count:,}")
                    
                    # Sample values
                    sample_values = str_series.head(3).tolist()
            else:
                inferred_type = f"UNKNOWN ({pandas_dtype})"
                sample_values = non_null_series.head(3).tolist()
                max_length = None
        
        # Show sample values
        print(f"   Sample values:")
        for i, val in enumerate(sample_values[:3], 1):
            val_str = str(val)
            if len(val_str) > 80:
                val_str = val_str[:77] + "..."
            print(f"      {i}. {val_str}")
        
        # Store results
        results.append({
            'column': col,
            'pandas_dtype': str(pandas_dtype),
            'inferred_type': inferred_type,
            'non_null': non_null_count,
            'null': null_count,
            'null_pct': null_percentage,
            'max_length': max_length
        })
    
    # Summary table
    print("\n" + "=" * 100)
    print("SUMMARY TABLE")
    print("=" * 100)
    print(f"{'Column Name':<40} {'Pandas Type':<15} {'Inferred Type':<25} {'Non-Null':<12} {'Null %':<10}")
    print("-" * 100)
    
    for r in results:
        col_name = r['column'][:38] + ".." if len(r['column']) > 40 else r['column']
        inferred = r['inferred_type'][:23] + ".." if len(r['inferred_type']) > 25 else r['inferred_type']
        print(f"{col_name:<40} {r['pandas_dtype']:<15} {inferred:<25} {r['non_null']:<12,} {r['null_pct']:<10.1f}")
    
    # PostgreSQL type recommendations
    print("\n" + "=" * 100)
    print("POSTGRESQL TYPE RECOMMENDATIONS")
    print("=" * 100)
    
    for r in results:
        col = r['column']
        inferred = r['inferred_type']
        max_len = r['max_length']
        
        if "DATETIME" in inferred:
            pg_type = "TIMESTAMP"
        elif "BOOLEAN" in inferred:
            pg_type = "BOOLEAN or BIT(1)"
        elif "INTEGER" in inferred and "string" not in inferred:
            pg_type = "INTEGER"
        elif "FLOAT" in inferred and "string" not in inferred:
            pg_type = "DOUBLE PRECISION"
        elif "VARCHAR" in inferred or "STRING" in inferred:
            if max_len and max_len <= 50:
                pg_type = f"VARCHAR({max_len + 10})"
            elif max_len and max_len <= 255:
                pg_type = f"VARCHAR({max_len + 20})"
            else:
                pg_type = "TEXT"
        else:
            pg_type = "TEXT"
        
        print(f"   {col:<40} -> {pg_type}")
    
    print("\n" + "=" * 100)
    print("âœ… Analysis complete!")
    print("=" * 100)

def main():
    """Main execution function."""
    if len(sys.argv) < 2:
        print("Usage: python analyze_csv_columns.py <csv_file_path>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    analyze_csv_columns(csv_file)

if __name__ == "__main__":
    main()
