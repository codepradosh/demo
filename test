#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sync gsnow created_on to PostgreSQL opened_at for INC1 incidents
Flow:
1. Fetch INC1 incident numbers from PostgreSQL (where opened_at is NULL)
2. For each incident, search in gsnow
3. Pull created_on from gsnow
4. Update opened_at in PostgreSQL with created_on value
"""

import os
import sys
import pandas as pd
import traceback
import logging
from datetime import datetime
from urllib.parse import quote_plus
from dotenv import load_dotenv
from pystarburst import Session
from trino.auth import OAuth2Authentication
from sqlalchemy import create_engine, text
from tqdm import tqdm

# Disable SSL warnings
import urllib3
urllib3.disable_warnings()

# ------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------
def setup_logging():
    """Set up logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

# ------------------------------------------------------------------------
# PostgreSQL Configuration
# ------------------------------------------------------------------------
POSTGRES_CONFIG = {
    'host': os.getenv('PGHOST', 'xd1e1159676dev.ubscloud-prod.msad.ubs.net'),
    'port': os.getenv('PGPORT', '5432'),
    'database': os.getenv('PGDATABASE', 'service_automation_db'),
    'user': os.getenv('PGUSER', 'powerbi_user'),
    'password': os.getenv('PGPASSWORD', 'Report@123'),
    'table_name': 'incident_data'
}

# ------------------------------------------------------------------------
# gsnow Configuration (from daily_load_incidents.py)
# ------------------------------------------------------------------------
GSNOW_CONNECTION_DETAILS = [
    {
        "connectionString": "Driver={Starburst ODBC Driver};Host=app-neu.starburst-prod.azpriv-cloud.ubs.net;Port=443;AuthenticationType=LDAP Authentication;SSL=1",
        "catalogName": "ts_dwh_gsnow",
        "schemaName": "ts_dwh_gsnow_schema",
        "tableName": "gsnow_inc_main_unstrreq_36mth",
        "dataproductId": "72567983-5ac4-405b-a6f8-6b9bd79244fe",
        "outputportId": "1709c349-7dae-492f-934e-2ce4bb5c5870"
    }
]

def create_gsnow_session():
    """Create PyStarburst session for gsnow catalog"""
    connection_details = GSNOW_CONNECTION_DETAILS
    
    session = None
    catalog_name = None
    
    for platform_connection_info in connection_details:
        if platform_connection_info.get("catalogName") == "ts_dwh_gsnow":
            connection_string_list = platform_connection_info["connectionString"].split(";")
            catalog_name = platform_connection_info["catalogName"]
            
            starburst_connection_details = {}
            for elem in connection_string_list:
                if "=" in elem:
                    key, value = elem.split("=", 1)
                    starburst_connection_details[key.strip()] = value.strip()
            
            host = starburst_connection_details.get("Host")
            port = starburst_connection_details.get("Port", "443")
            
            db_parameters = {
                "host": host,
                "port": port,
                "http_scheme": "https",
                "verify": False,
                "auth": OAuth2Authentication(),
                "catalog": catalog_name
            }
            session = Session.builder.configs(db_parameters).create()
            break
    
    return session, catalog_name

def get_inc1_incidents_from_postgres(engine, table_name, incident_prefix='INC1', limit=None):
    """
    Fetch INC1 incident numbers from PostgreSQL where opened_at is NULL.
    
    Parameters
    ----------
    engine : sqlalchemy.engine.Engine
        SQLAlchemy engine
    table_name : str
        Table name
    incident_prefix : str
        Incident number prefix (default: 'INC1')
    limit : int, optional
        Maximum number of incidents to fetch (default: None = all)
        
    Returns
    -------
    pd.DataFrame
        DataFrame with incident_number and sys_created_on columns
    """
    query = f"""
    SELECT 
        incident_number,
        sys_created_on
    FROM "{table_name}"
    WHERE data_source = 'gsnow'
      AND opened_at IS NULL
      AND incident_number LIKE '{incident_prefix}%'
    ORDER BY incident_number
    """
    
    if limit:
        query += f" LIMIT {limit}"
    
    df = pd.read_sql(query, engine)
    return df

def get_created_on_from_gsnow(session, incident_number):
    """
    Get created_on value from gsnow for a specific incident number.
    
    Parameters
    ----------
    session : pystarburst.Session
        gsnow session
    incident_number : str
        Incident number to search
        
    Returns
    -------
    datetime or None
        created_on value or None if not found
    """
    schema = "ts_dwh_gsnow_schema"
    table = "gsnow_inc_main_unstrreq_36mth"
    table_ref = f"{schema}.{table}"
    
    query = f"""
    SELECT created_on
    FROM {table_ref}
    WHERE inc_number = '{incident_number}'
    LIMIT 1;
    """
    
    try:
        df = session.sql(query).to_pandas()
        if len(df) > 0 and df['created_on'].notna().iloc[0]:
            return pd.to_datetime(df['created_on'].iloc[0])
        return None
    except Exception as e:
        logging.warning(f"   ‚ö†Ô∏è  Error fetching created_on for {incident_number}: {e}")
        return None

def update_opened_at_in_postgres(engine, table_name, incident_number, created_on_value):
    """
    Update opened_at in PostgreSQL with created_on value.
    
    Parameters
    ----------
    engine : sqlalchemy.engine.Engine
        SQLAlchemy engine
    table_name : str
        Table name
    incident_number : str
        Incident number to update
    created_on_value : datetime
        created_on value to use for opened_at
        
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    try:
        # Escape single quotes in incident_number for safety
        safe_incident_number = incident_number.replace("'", "''")
        
        # Format the datetime value properly
        if created_on_value is None:
            return False
        
        # Convert to string format that PostgreSQL understands
        if isinstance(created_on_value, pd.Timestamp):
            created_on_str = created_on_value.strftime('%Y-%m-%d %H:%M:%S')
        elif isinstance(created_on_value, datetime):
            created_on_str = created_on_value.strftime('%Y-%m-%d %H:%M:%S')
        else:
            created_on_str = str(created_on_value)
        
        # Use parameterized query - simpler approach that works with both SQLAlchemy 1.4 and 2.0
        update_query = text(f"""
        UPDATE "{table_name}"
        SET opened_at = :created_on_value::timestamp,
            updated_at = CURRENT_TIMESTAMP
        WHERE incident_number = :incident_number
          AND data_source = 'gsnow'
        """)
        
        with engine.connect() as conn:
            # Pass parameters as keyword arguments (SQLAlchemy 2.0 style)
            result = conn.execute(
                update_query,
                created_on_value=created_on_str,
                incident_number=incident_number
            )
            conn.commit()
            return result.rowcount > 0
    except TypeError:
        # Fallback for SQLAlchemy 1.4 style (parameters as dict)
        try:
            safe_incident_number = incident_number.replace("'", "''")
            
            if isinstance(created_on_value, pd.Timestamp):
                created_on_str = created_on_value.strftime('%Y-%m-%d %H:%M:%S')
            elif isinstance(created_on_value, datetime):
                created_on_str = created_on_value.strftime('%Y-%m-%d %H:%M:%S')
            else:
                created_on_str = str(created_on_value)
            
            update_query = text(f"""
            UPDATE "{table_name}"
            SET opened_at = :created_on_value::timestamp,
                updated_at = CURRENT_TIMESTAMP
            WHERE incident_number = :incident_number
              AND data_source = 'gsnow'
            """)
            
            with engine.connect() as conn:
                result = conn.execute(
                    update_query,
                    {"created_on_value": created_on_str, "incident_number": incident_number}
                )
                conn.commit()
                return result.rowcount > 0
        except Exception as e2:
            logging.error(f"   ‚ùå Error updating {incident_number}: {e2}")
            traceback.print_exc()
            return False
    except Exception as e:
        logging.error(f"   ‚ùå Error updating {incident_number}: {e}")
        traceback.print_exc()
        return False

def sync_gsnow_inc1_to_postgres(table_name=None, database=None, 
                                user=None, host=None, port=None, password=None,
                                incident_prefix='INC1', limit=None, dry_run=True):
    """
    Main function to sync gsnow created_on to PostgreSQL opened_at.
    
    Parameters
    ----------
    table_name : str, optional
        Target table name (default: 'incident_data')
    database : str, optional
        Database name (default: from config)
    user : str, optional
        PostgreSQL username (default: from config)
    host : str, optional
        PostgreSQL host (default: from config)
    port : str, optional
        PostgreSQL port (default: from config)
    password : str, optional
        PostgreSQL password (default: from environment variable)
    incident_prefix : str, optional
        Incident number prefix (default: 'INC1')
    limit : int, optional
        Maximum number of incidents to process (default: None = all)
    dry_run : bool, optional
        If True, only show what would be updated (default: True)
        
    Returns
    -------
    bool
        True if successful, False otherwise
    """
    try:
        logging.info("="*80)
        logging.info("üîÑ SYNC GSNOW created_on TO POSTGRESQL opened_at")
        logging.info("="*80)
        logging.info(f"   Database: {database or POSTGRES_CONFIG['database']}")
        logging.info(f"   Table: {table_name or POSTGRES_CONFIG['table_name']}")
        logging.info(f"   Incident prefix: {incident_prefix}")
        logging.info(f"   Limit: {limit or 'All'}")
        logging.info(f"   Mode: {'DRY RUN (no changes)' if dry_run else 'LIVE UPDATE'}")
        logging.info("="*80)
        
        # Connect to PostgreSQL
        table_name = (table_name or POSTGRES_CONFIG['table_name']).lower()
        database = database or POSTGRES_CONFIG['database']
        user = user or POSTGRES_CONFIG['user']
        host = host or POSTGRES_CONFIG['host']
        port = port or POSTGRES_CONFIG['port']
        password = password or POSTGRES_CONFIG['password']
        
        logging.info(f"\n‚è≥ Connecting to PostgreSQL...")
        if password:
            encoded_password = quote_plus(password)
            pg_connection_string = f"postgresql://{user}:{encoded_password}@{host}:{port}/{database}"
        else:
            pg_connection_string = f"postgresql://{user}@{host}:{port}/{database}"
        
        pg_engine = create_engine(pg_connection_string, pool_pre_ping=True)
        
        with pg_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logging.info(f"‚úÖ Connected to PostgreSQL")
        
        # Connect to gsnow
        logging.info(f"\n‚è≥ Connecting to gsnow...")
        gsnow_session, catalog_name = create_gsnow_session()
        
        if not gsnow_session:
            logging.error("‚ùå Failed to create gsnow session")
            pg_engine.dispose()
            return False
        
        logging.info(f"‚úÖ Connected to gsnow catalog: {catalog_name}")
        
        # Step 1: Fetch INC1 incidents from PostgreSQL
        logging.info(f"\n{'='*80}")
        logging.info("STEP 1: Fetching INC1 incidents from PostgreSQL")
        logging.info(f"{'='*80}")
        
        df_incidents = get_inc1_incidents_from_postgres(
            pg_engine, 
            table_name, 
            incident_prefix=incident_prefix,
            limit=limit
        )
        
        if len(df_incidents) == 0:
            logging.info(f"‚úì No incidents found with NULL opened_at and prefix '{incident_prefix}'")
            pg_engine.dispose()
            return True
        
        logging.info(f"‚úì Found {len(df_incidents):,} incidents to process")
        logging.info(f"   Sample incident numbers: {', '.join(df_incidents['incident_number'].head(10).tolist())}")
        
        # Step 2: Process each incident one by one
        logging.info(f"\n{'='*80}")
        logging.info("STEP 2: Processing incidents one by one")
        logging.info(f"{'='*80}")
        logging.info(f"   Fetching created_on from gsnow for each incident...")
        
        results = {
            'total': len(df_incidents),
            'found_in_gsnow': 0,
            'not_found_in_gsnow': 0,
            'updated': 0,
            'failed': 0,
            'skipped': 0
        }
        
        # Process with progress bar
        try:
            from tqdm import tqdm
            progress_bar = tqdm(df_incidents.iterrows(), total=len(df_incidents), desc="Processing", unit="incident")
        except ImportError:
            progress_bar = df_incidents.iterrows()
            logging.info("   (Install tqdm for progress bar: pip install tqdm)")
        
        for idx, row in progress_bar:
            incident_number = row['incident_number']
            sys_created_on = row['sys_created_on']
            
            # Step 2a: Search in gsnow
            created_on_value = get_created_on_from_gsnow(gsnow_session, incident_number)
            
            if created_on_value is None:
                results['not_found_in_gsnow'] += 1
                if idx < 5:  # Log first few
                    logging.warning(f"   ‚ö†Ô∏è  {incident_number}: Not found in gsnow")
                continue
            
            results['found_in_gsnow'] += 1
            
            # Step 2b: Update PostgreSQL
            if not dry_run:
                success = update_opened_at_in_postgres(
                    pg_engine,
                    table_name,
                    incident_number,
                    created_on_value
                )
                
                if success:
                    results['updated'] += 1
                    if idx < 5:  # Log first few
                        logging.info(f"   ‚úÖ {incident_number}: Updated opened_at = {created_on_value}")
                else:
                    results['failed'] += 1
                    if idx < 5:  # Log first few
                        logging.error(f"   ‚ùå {incident_number}: Failed to update")
            else:
                results['skipped'] += 1
                if idx < 5:  # Log first few
                    logging.info(f"   üîç {incident_number}: Would update opened_at = {created_on_value}")
        
        # Summary
        logging.info(f"\n{'='*80}")
        logging.info("‚úÖ SYNC COMPLETED")
        logging.info(f"{'='*80}")
        logging.info(f"üìä Summary:")
        logging.info(f"   Total incidents processed: {results['total']:,}")
        logging.info(f"   Found in gsnow: {results['found_in_gsnow']:,}")
        logging.info(f"   Not found in gsnow: {results['not_found_in_gsnow']:,}")
        
        if dry_run:
            logging.info(f"   Would update: {results['skipped']:,}")
            logging.info(f"\n   Run with --execute to apply changes")
        else:
            logging.info(f"   Successfully updated: {results['updated']:,}")
            logging.info(f"   Failed to update: {results['failed']:,}")
        
        logging.info(f"{'='*80}\n")
        
        pg_engine.dispose()
        return True
        
    except Exception as e:
        logging.error(f"\n‚úó Failed to sync: {e}")
        logging.error(traceback.format_exc())
        return False

def main():
    """Main execution function."""
    import argparse
    
    setup_logging()
    
    parser = argparse.ArgumentParser(
        description='Sync gsnow created_on to PostgreSQL opened_at for INC1 incidents',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Flow:
1. Fetch INC1 incident numbers from PostgreSQL (where opened_at is NULL)
2. For each incident, search in gsnow
3. Pull created_on from gsnow
4. Update opened_at in PostgreSQL with created_on value

Examples:
  # Dry run (show what would be updated, no changes)
  python sync_gsnow_inc1_to_postgres.py
  
  # Dry run with limit
  python sync_gsnow_inc1_to_postgres.py --limit 100
  
  # Execute the update
  python sync_gsnow_inc1_to_postgres.py --execute
  
  # Execute with custom prefix and limit
  python sync_gsnow_inc1_to_postgres.py --execute --prefix INC2 --limit 500

Environment variables (optional):
  PGHOST=localhost
  PGPORT=5432
  PGDATABASE=service_automation_db
  PGUSER=postgres
  PGPASSWORD=your_password
        """
    )
    
    parser.add_argument('--execute', action='store_true',
                       help='Execute the update (default: dry run mode)')
    parser.add_argument('--table', type=str, default=None,
                       help='Table name (default: incident_data)')
    parser.add_argument('--database', type=str, default=None,
                       help='Database name (default: from config)')
    parser.add_argument('--prefix', type=str, default='INC1',
                       help='Incident number prefix (default: INC1)')
    parser.add_argument('--limit', type=int, default=None,
                       help='Maximum number of incidents to process (default: all)')
    
    args = parser.parse_args()
    
    # Sync
    success = sync_gsnow_inc1_to_postgres(
        table_name=args.table,
        database=args.database,
        incident_prefix=args.prefix,
        limit=args.limit,
        dry_run=not args.execute
    )
    
    if success:
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()
