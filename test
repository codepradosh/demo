#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV Column Data Type Analyzer
Analyzes a CSV file and displays data types, sample values, and statistics for each column.
Usage: python analyze_csv_columns.py <csv_file_path>
"""

import os
import sys
import pandas as pd
import numpy as np
from collections import Counter

def analyze_csv_columns(csv_file_path):
    """
    Analyze CSV file and display column information including data types.
    
    Parameters
    ----------
    csv_file_path : str
        Path to the CSV file to analyze
    """
    if not os.path.exists(csv_file_path):
        print(f"âŒ CSV file not found: {csv_file_path}")
        return
    
    print("=" * 100)
    print("ðŸ“Š CSV COLUMN DATA TYPE ANALYZER")
    print("=" * 100)
    print(f"ðŸ“ File: {csv_file_path}\n")
    
    # Read CSV file
    print("â³ Reading CSV file...")
    try:
        df = pd.read_csv(csv_file_path, encoding='utf-8', low_memory=False)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(csv_file_path, encoding='latin-1', low_memory=False)
            print("   âš ï¸  Used latin-1 encoding")
        except:
            df = pd.read_csv(csv_file_path, encoding='cp1252', low_memory=False)
            print("   âš ï¸  Used cp1252 encoding")
    
    print(f"âœ… Read {len(df):,} rows, {len(df.columns)} columns\n")
    
    # Analyze each column
    print("=" * 100)
    print("COLUMN ANALYSIS")
    print("=" * 100)
    
    results = []
    
    for idx, col in enumerate(df.columns, 1):
        print(f"\n[{idx}/{len(df.columns)}] Column: '{col}'")
        print("-" * 100)
        
        # Basic info
        total_count = len(df[col])
        non_null_count = df[col].notna().sum()
        null_count = total_count - non_null_count
        null_percentage = (null_count / total_count * 100) if total_count > 0 else 0
        
        print(f"   Total rows: {total_count:,}")
        print(f"   Non-null: {non_null_count:,} ({100-null_percentage:.1f}%)")
        print(f"   Null/Empty: {null_count:,} ({null_percentage:.1f}%)")
        
        # Pandas dtype
        pandas_dtype = df[col].dtype
        print(f"   Pandas dtype: {pandas_dtype}")
        
        # Actual data type analysis
        non_null_series = df[col].dropna()
        
        if len(non_null_series) == 0:
            print(f"   âš ï¸  All values are NULL/Empty")
            inferred_type = "UNKNOWN (all null)"
            sample_values = ["N/A"]
        else:
            # Check for datetime
            is_datetime = pd.api.types.is_datetime64_any_dtype(df[col])
            if is_datetime:
                inferred_type = "DATETIME"
                sample_values = non_null_series.head(3).astype(str).tolist()
                print(f"   âœ… Detected as: DATETIME")
            # Check for boolean
            elif pandas_dtype == 'bool' or (pandas_dtype == 'object' and 
                    non_null_series.astype(str).str.lower().isin(['true', 'false', '1', '0', 'yes', 'no']).all()):
                inferred_type = "BOOLEAN"
                unique_vals = non_null_series.unique()[:5]
                sample_values = [str(v) for v in unique_vals]
                print(f"   âœ… Detected as: BOOLEAN")
            # Check for integer
            elif pd.api.types.is_integer_dtype(df[col]):
                inferred_type = "INTEGER"
                min_val = non_null_series.min()
                max_val = non_null_series.max()
                sample_values = non_null_series.head(3).tolist()
                print(f"   âœ… Detected as: INTEGER")
                print(f"   Range: {min_val} to {max_val}")
            # Check for float
            elif pd.api.types.is_float_dtype(df[col]):
                inferred_type = "FLOAT"
                min_val = non_null_series.min()
                max_val = non_null_series.max()
                sample_values = non_null_series.head(3).tolist()
                print(f"   âœ… Detected as: FLOAT")
                print(f"   Range: {min_val:.2f} to {max_val:.2f}")
            # Check if numeric string
            elif pandas_dtype == 'object':
                # Try to convert to numeric
                numeric_series = pd.to_numeric(non_null_series, errors='coerce')
                numeric_count = numeric_series.notna().sum()
                
                if numeric_count == len(non_null_series):
                    # All numeric
                    if numeric_series.dtype in ['int64', 'Int64']:
                        inferred_type = "INTEGER (string)"
                        min_val = numeric_series.min()
                        max_val = numeric_series.max()
                        print(f"   âœ… Detected as: INTEGER (stored as string)")
                        print(f"   Range: {min_val} to {max_val}")
                    else:
                        inferred_type = "FLOAT (string)"
                        min_val = numeric_series.min()
                        max_val = numeric_series.max()
                        print(f"   âœ… Detected as: FLOAT (stored as string)")
                        print(f"   Range: {min_val:.2f} to {max_val:.2f}")
                    sample_values = non_null_series.head(3).tolist()
                else:
                    # String type
                    str_series = non_null_series.astype(str)
                    max_length = str_series.str.len().max()
                    min_length = str_series.str.len().min()
                    avg_length = str_series.str.len().mean()
                    
                    # Check if it looks like a date string
                    date_like = False
                    if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'updated', 'opened', 'closed', 'resolved']):
                        date_like = True
                        # Try to parse as date
                        try:
                            test_dates = pd.to_datetime(str_series.head(100), errors='coerce')
                            if test_dates.notna().sum() > len(test_dates) * 0.8:  # 80% parseable
                                inferred_type = "DATETIME (string)"
                                print(f"   âœ… Detected as: DATETIME (stored as string)")
                                date_like = True
                        except:
                            pass
                    
                    if not date_like:
                        inferred_type = "STRING/VARCHAR"
                        print(f"   âœ… Detected as: STRING/VARCHAR")
                    
                    print(f"   String length: min={min_length}, max={max_length}, avg={avg_length:.1f}")
                    
                    # Show unique value count
                    unique_count = non_null_series.nunique()
                    print(f"   Unique values: {unique_count:,}")
                    
                    # Sample values
                    sample_values = non_null_series.head(3).tolist()
                    
                    # Show most common values if not too many unique
                    if unique_count <= 20 and unique_count > 0:
                        print(f"   Most common values:")
                        value_counts = non_null_series.value_counts().head(5)
                        for val, count in value_counts.items():
                            print(f"      '{val}': {count:,} ({count/len(non_null_series)*100:.1f}%)")
            else:
                inferred_type = f"UNKNOWN ({pandas_dtype})"
                sample_values = non_null_series.head(3).tolist()
        
        # Show sample values
        print(f"   Sample values:")
        for i, val in enumerate(sample_values[:3], 1):
            val_str = str(val)
            if len(val_str) > 80:
                val_str = val_str[:77] + "..."
            print(f"      {i}. {val_str}")
        
        # Store results
        results.append({
            'column': col,
            'pandas_dtype': str(pandas_dtype),
            'inferred_type': inferred_type,
            'non_null': non_null_count,
            'null': null_count,
            'null_pct': null_percentage,
            'max_length': max_length if inferred_type == "STRING/VARCHAR" else None
        })
    
    # Summary table
    print("\n" + "=" * 100)
    print("SUMMARY TABLE")
    print("=" * 100)
    print(f"{'Column Name':<40} {'Pandas Type':<15} {'Inferred Type':<20} {'Non-Null':<12} {'Null %':<10}")
    print("-" * 100)
    
    for r in results:
        col_name = r['column'][:38] + ".." if len(r['column']) > 40 else r['column']
        print(f"{col_name:<40} {r['pandas_dtype']:<15} {r['inferred_type']:<20} {r['non_null']:<12,} {r['null_pct']:<10.1f}")
    
    # PostgreSQL type recommendations
    print("\n" + "=" * 100)
    print("POSTGRESQL TYPE RECOMMENDATIONS")
    print("=" * 100)
    
    for r in results:
        col = r['column']
        inferred = r['inferred_type']
        max_len = r['max_length']
        
        if inferred == "DATETIME" or inferred == "DATETIME (string)":
            pg_type = "TIMESTAMP"
        elif inferred == "BOOLEAN":
            pg_type = "BOOLEAN or BIT(1)"
        elif inferred == "INTEGER" or inferred == "INTEGER (string)":
            pg_type = "INTEGER"
        elif inferred == "FLOAT" or inferred == "FLOAT (string)":
            pg_type = "DOUBLE PRECISION"
        elif inferred == "STRING/VARCHAR":
            if max_len and max_len <= 50:
                pg_type = f"VARCHAR({max_len + 10})"
            elif max_len and max_len <= 255:
                pg_type = f"VARCHAR({max_len + 20})"
            else:
                pg_type = "TEXT"
        else:
            pg_type = "TEXT"
        
        print(f"   {col:<40} -> {pg_type}")
    
    print("\n" + "=" * 100)
    print("âœ… Analysis complete!")
    print("=" * 100)

def main():
    """Main execution function."""
    if len(sys.argv) < 2:
        print("Usage: python analyze_csv_columns.py <csv_file_path>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    analyze_csv_columns(csv_file)

if __name__ == "__main__":
    main()
